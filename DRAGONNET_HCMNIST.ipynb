{"cells":[{"cell_type":"code","execution_count":null,"id":"05341e16","metadata":{"id":"05341e16"},"outputs":[],"source":["import numpy as np\n","import sys\n","import tensorflow as tf\n","from keras.losses import Loss\n","from sklearn.preprocessing import StandardScaler\n","from scipy.sparse import csr_matrix\n","import pandas as pd\n","import datetime\n","import time\n","import io\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"32e0d37a","metadata":{"id":"32e0d37a"},"outputs":[],"source":["import torch\n","import typing\n","\n","from pathlib import Path\n","\n","from torchvision import datasets\n","\n","from sklearn import model_selection\n","\n","\n","def lambda_top_func(mu, k, y, alpha):\n","    m = y.shape[0]\n","    r = (y[k:] - mu).sum(dim=0)\n","    return mu + r.div(m * (alpha + 1) - k)\n","\n","\n","def lambda_bottom_func(mu, k, y, alpha):\n","    m = y.shape[0]\n","    r = (y[:k] - mu).sum(dim=0)\n","    return mu + r.div(m * alpha + k)\n","\n","\n","def alpha_fn(pi, lambda_):\n","    return (pi * lambda_) ** -1 + 1.0 - lambda_ ** -1\n","\n","\n","def beta_fn(pi, lambda_):\n","    return lambda_ * (pi) ** -1 + 1.0 - lambda_\n","\n","\n","def policy_risk(pi, y1, y0):\n","    return (pi * y1 + (1 - pi) * y0).mean()\n","\n","def complete_propensity(x, u, gamma, beta=0.75):\n","    logit = beta * x + 0.5\n","    nominal = (1 + np.exp(-logit)) ** -1\n","    alpha = alpha_fn(nominal, gamma)\n","    beta = beta_fn(nominal, gamma)\n","    return (u / alpha) + ((1 - u) / beta)\n","\n","\n","def f_mu(x, t, u, theta=4.0):\n","    mu = (\n","        (2 * t - 1) * x\n","        + (2.0 * t - 1)\n","        - 2 * np.sin((4 * t - 2) * x)\n","        - (theta * u - 2) * (1 + 0.5 * x)\n","    )\n","    return mu\n","\n","\n","def linear_normalization(x, new_min, new_max):\n","    return (x - x.min()) * (new_max - new_min) / (x.max() - x.min()) + new_min\n","\n","\n","class HCMNIST(datasets.MNIST):\n","    def __init__(\n","        self,\n","        root: str,\n","        gamma_star: float,\n","        split: str = \"train\",\n","        mode: str = \"mu\",\n","        p_u: str = \"bernoulli\",\n","        theta: float = 4.0,\n","        beta: float = 0.75,\n","        sigma_y: float = 1.0,\n","        domain: float = 2.0,\n","        seed: int = 1331,\n","        transform: typing.Optional[typing.Callable] = None,\n","        target_transform: typing.Optional[typing.Callable] = None,\n","        download: bool = True,\n","    ) -> None:\n","        train = split == \"train\" or split == \"valid\"\n","        root = Path.home() / \"quince_datasets\" if root is None else Path(root)\n","        self.__class__.__name__ = \"MNIST\"\n","        super(HCMNIST, self).__init__(\n","            root,\n","            train=train,\n","            transform=transform,\n","            target_transform=target_transform,\n","            download=download,\n","        )\n","        self.data = self.data.view(len(self.targets), -1).numpy()\n","        self.targets = self.targets.numpy()\n","\n","        if train:\n","            (\n","                data_train,\n","                data_valid,\n","                targets_train,\n","                targets_valid,\n","            ) = model_selection.train_test_split(\n","                self.data, self.targets, test_size=0.3, random_state=seed\n","            )\n","            self.data = data_train if split == \"train\" else data_valid\n","            self.targets = targets_train if split == \"train\" else targets_valid\n","\n","        self.mode = mode\n","        self.dim_input = [1, 28, 28]\n","        self.dim_treatment = 1\n","        self.dim_output = 1\n","\n","        self.phi_model = fit_phi_model(\n","            root=root, edges=torch.arange(-domain, domain + 0.1, (2 * domain) / 10),\n","        )\n","\n","        size = (self.__len__(), 1)\n","        rng = np.random.RandomState(seed=seed)\n","        if p_u == \"bernoulli\":\n","            self.u = rng.binomial(1, 0.5, size=size).astype(\"float32\")\n","        elif p_u == \"uniform\":\n","            self.u = rng.uniform(size=size).astype(\"float32\")\n","        elif p_u == \"beta_bi\":\n","            self.u = rng.beta(0.5, 0.5, size=size).astype(\"float32\")\n","        elif p_u == \"beta_uni\":\n","            self.u = rng.beta(2, 5, size=size).astype(\"float32\")\n","        else:\n","            raise NotImplementedError(f\"{p_u} is not a supported distribution\")\n","\n","        phi = self.phi\n","        self.pi = (\n","            complete_propensity(x=phi, u=self.u, gamma=gamma_star, beta=beta)\n","            .astype(\"float32\")\n","            .ravel()\n","        )\n","        self.t = rng.binomial(1, self.pi).astype(\"float32\")\n","        eps = (sigma_y * rng.normal(size=self.t.shape)).astype(\"float32\")\n","        self.mu0 = (\n","            f_mu(x=phi, t=0.0, u=self.u, theta=theta).astype(\"float32\").ravel()\n","        )\n","        self.mu1 = (\n","            f_mu(x=phi, t=1.0, u=self.u, theta=theta).astype(\"float32\").ravel()\n","        )\n","        self.y0 = self.mu0 + eps\n","        self.y1 = self.mu1 + eps\n","        self.y = self.t * self.y1 + (1 - self.t) * self.y0\n","        self.tau = self.mu1 - self.mu0\n","        self.y_mean = np.array([0.0], dtype=\"float32\")\n","        self.y_std = np.array([1.0], dtype=\"float32\")\n","\n","    def __getitem__(self, index):\n","        x = ((self.data[index].astype(\"float32\") / 255.0) - 0.1307) / 0.3081\n","        t = self.t[index : index + 1]\n","        if self.mode == \"pi\":\n","            return x, t\n","        elif self.mode == \"mu\":\n","            return np.hstack([x, t]), self.y[index : index + 1]\n","        else:\n","            raise NotImplementedError(\n","                f\"{self.mode} not supported. Choose from 'pi'  for propensity models or 'mu' for expected outcome models\"\n","            )\n","\n","    @property\n","    def phi(self):\n","        x = ((self.data.astype(\"float32\") / 255.0) - 0.1307) / 0.3081\n","        z = np.zeros_like(self.targets.astype(\"float32\"))\n","        for k, v in self.phi_model.items():\n","            ind = self.targets == k\n","            x_ind = x[ind].reshape(ind.sum(), -1)\n","            means = x_ind.mean(axis=-1)\n","            z[ind] = linear_normalization(\n","                np.clip((means - v[\"mu\"]) / v[\"sigma\"], -1.4, 1.4), v[\"lo\"], v[\"hi\"]\n","            )\n","        return np.expand_dims(z, -1)\n","\n","    @property\n","    def x(self):\n","        return ((self.data.astype(\"float32\") / 255.0) - 0.1307) / 0.3081\n","\n","\n","def fit_phi_model(root, edges):\n","    ds = datasets.MNIST(root=root)\n","    data = (ds.data.float().div(255) - 0.1307).div(0.3081).view(len(ds), -1)\n","    model = {}\n","    digits = torch.unique(ds.targets)\n","    for i, digit in enumerate(digits):\n","        lo, hi = edges[i : i + 2]\n","        ind = ds.targets == digit\n","        data_ind = data[ind].view(ind.sum(), -1)\n","        means = data_ind.mean(dim=-1)\n","        mu = means.mean()\n","        sigma = means.std()\n","        model.update(\n","            {\n","                digit.item(): {\n","                    \"mu\": mu.item(),\n","                    \"sigma\": sigma.item(),\n","                    \"lo\": lo.item(),\n","                    \"hi\": hi.item(),\n","                }\n","            }\n","        )\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"db01ebab","metadata":{"id":"db01ebab"},"outputs":[],"source":["def get_HMINIST_data(sim_num):\n","  obj = HCMNIST(root = \"./\",gamma_star = 1.1, seed=sim_num)\n","  # Assuming X is a matrix of size (42000, 784)\n","  # Assuming indices is the array of generated indices\n","  indices = np.random.choice(42000, size=3000, replace=False)\n","  # Select subrows from X using the generated indices\n","\n","  X = obj.x[indices]\n","  Y_factual = obj.y[indices]\n","  T = obj.t[indices]\n","  mu_0 = obj.mu0[indices]\n","  mu_1 = obj.mu1[indices]\n","\n","  X_train, X_test, Y_factual_train, _, T_train, _, mu_0_train, mu_0_test, mu_1_train, mu_1_test = train_test_split(X, Y_factual, T, mu_0, mu_1, test_size=0.1, random_state=sim_num)\n","\n","  X_train = X_train.astype('float32')\n","  Y_factual_train = Y_factual_train.astype('float32') #most GPUs only compute 32-bit floats\n","  T_train = T_train.astype('float32')\n","  mu_0_train = mu_0_train.astype('float32')\n","  mu_1_train = mu_1_train.astype('float32')\n","\n","  X_test = X_test.astype('float32')\n","  mu_0_test = mu_0_test.astype('float32')\n","  mu_1_test = mu_1_test.astype('float32')\n","\n","  data_train={'x':X_train,'y':Y_factual_train,'t':T_train,'mu_0':mu_0_train,'mu_1':mu_1_train}\n","  data_train['t']=data_train['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","  data_train['y']=data_train['y'].reshape(-1,1)\n","  data_train['mu_0']=data_train['mu_0'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","  data_train['mu_1']=data_train['mu_1'].reshape(-1,1)\n","\n","  #rescaling y between 0 and 1 often makes training of DL regressors easier\n","  data_train['y_scaler'] = StandardScaler().fit(data_train['y'])\n","  data_train['ys'] = data_train['y_scaler'].transform(data_train['y'])\n","\n","  data_test={'x':X_test,'mu_0':mu_0_test,'mu_1':mu_1_test}\n","  data_test['y_scaler'] = data_train['y_scaler']\n","  data_test['mu_0']=data_test['mu_0'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension\n","  data_test['mu_1']=data_test['mu_1'].reshape(-1,1)\n","\n","  return data_train, data_test"]},{"cell_type":"code","execution_count":null,"id":"ab1741a4","metadata":{"id":"ab1741a4"},"outputs":[],"source":["from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import Model\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.metrics import binary_accuracy\n","from tensorflow.keras.losses import Loss\n","class EpsilonLayer(Layer):\n","\n","    def __init__(self):\n","        super(EpsilonLayer, self).__init__()\n","\n","    def build(self, input_shape):\n","        # Create a trainable weight variable for this layer.\n","        self.epsilon = self.add_weight(name='epsilon',\n","                                       shape=[1, 1],\n","                                       initializer='RandomNormal',\n","                                       #  initializer='ones',\n","                                       trainable=True)\n","        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, inputs, **kwargs):\n","        #note there is only one epsilon were just duplicating it for conformability\n","        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n","\n","def make_dragonnet(input_dim, reg_l2):\n","\n","    x = Input(shape=(input_dim,), name='input')\n","    # representation\n","    phi = Dense(units=512, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n","    phi = Dense(units=256, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n","    phi = Dense(units=128, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n","\n","    # HYPOTHESIS\n","    y0_hidden = Dense(units=64, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n","    y1_hidden = Dense(units=32, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n","\n","    # second layer\n","    y0_hidden = Dense(units=64, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n","    y1_hidden = Dense(units=32, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n","\n","    # third\n","    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n","    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n","\n","    #propensity prediction\n","    #Note that the activation is actually sigmoid, but we will squish it in the loss function for numerical stability reasons\n","    t_predictions = Dense(units=1,activation=None,name='t_prediction')(phi)\n","    #Although the epsilon layer takes an input, it really just houses a free parameter.\n","    epsilons = EpsilonLayer()(t_predictions)\n","    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_predictions,epsilons,phi])\n","    model = Model(inputs=x, outputs=concat_pred)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"id":"efd96b67","metadata":{"id":"efd96b67"},"outputs":[],"source":["class Base_Loss(Loss):\n","    #initialize instance attributes\n","    def __init__(self, alpha=1.0):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.name='standard_loss'\n","\n","    def split_pred(self,concat_pred):\n","        #generic helper to make sure we dont make mistakes\n","        preds={}\n","        preds['y0_pred'] = concat_pred[:, 0]\n","        preds['y1_pred'] = concat_pred[:, 1]\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['phi'] = concat_pred[:, 3:]\n","        return preds\n","\n","    #for logging purposes only\n","    def treatment_acc(self,concat_true,concat_pred):\n","        t_true = concat_true[:, 1]\n","        p = self.split_pred(concat_pred)\n","        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n","        return tf.reduce_mean(binary_accuracy(t_true, tf.math.sigmoid(p['t_pred']), threshold=0.5))\n","\n","    def treatment_bce(self,concat_true,concat_pred):\n","        t_true = concat_true[:, 1]\n","        p = self.split_pred(concat_pred)\n","        lossP = tf.reduce_sum(binary_crossentropy(t_true,p['t_pred'],from_logits=True))\n","        return lossP\n","\n","    def regression_loss(self,concat_true,concat_pred):\n","        y_true = concat_true[:, 0]\n","        t_true = concat_true[:, 1]\n","        p = self.split_pred(concat_pred)\n","        loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - p['y0_pred']))\n","        loss1 = tf.reduce_sum(t_true * tf.square(y_true - p['y1_pred']))\n","        return loss0+loss1\n","\n","    def standard_loss(self,concat_true,concat_pred):\n","        lossR = self.regression_loss(concat_true,concat_pred)\n","        lossP = self.treatment_bce(concat_true,concat_pred)\n","        return lossR + self.alpha * lossP\n","\n","    #compute loss\n","    def call(self, concat_true, concat_pred):\n","        return self.standard_loss(concat_true,concat_pred)"]},{"cell_type":"code","execution_count":null,"id":"81c8c560","metadata":{"id":"81c8c560"},"outputs":[],"source":["class TarReg_Loss(Base_Loss):\n","    #initialize instance attributes\n","    def __init__(self, alpha=1,beta=1):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.beta=beta\n","        self.name='tarreg_loss'\n","\n","    def split_pred(self,concat_pred):\n","        #generic helper to make sure we dont make mistakes\n","        preds={}\n","        preds['y0_pred'] = concat_pred[:, 0]\n","        preds['y1_pred'] = concat_pred[:, 1]\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['epsilon'] = concat_pred[:, 3] #we're moving epsilon into slot three\n","        preds['phi'] = concat_pred[:, 4:]\n","        return preds\n","\n","    def calc_hstar(self,concat_true,concat_pred):\n","        #step 2 above\n","        p=self.split_pred(concat_pred)\n","        y_true = concat_true[:, 0]\n","        t_true = concat_true[:, 1]\n","\n","        t_pred = tf.math.sigmoid(concat_pred[:, 2])\n","        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n","        y_pred = t_true * p['y1_pred'] + (1 - t_true) * p['y0_pred']\n","\n","        #calling it cc for \"clever covariate\" as in SuperLearner TMLE literature\n","        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n","        h_star = y_pred + p['epsilon'] * cc\n","        return h_star\n","\n","    def call(self,concat_true,concat_pred):\n","        y_true = concat_true[:, 0]\n","\n","        standard_loss=self.standard_loss(concat_true,concat_pred)\n","        h_star=self.calc_hstar(concat_true,concat_pred)\n","        #step 3 above\n","        targeted_regularization = tf.reduce_sum(tf.square(y_true - h_star))\n","\n","        # final\n","        loss = standard_loss + self.beta * targeted_regularization\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"64d95324","metadata":{"id":"64d95324"},"outputs":[],"source":["\n","#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n","class Eval_metrics_train():\n","    def __init__(self,data):\n","        self.data=data #feed the callback the full dataset\n","        #needed for PEHEnn; Called in self.find_ynn\n","        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n","        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n","        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n","\n","    def split_pred(self,concat_pred):\n","        preds={}\n","        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n","        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['epsilon'] = concat_pred[:, 3]\n","        preds['phi'] = concat_pred[:, 4:]\n","        return preds\n","\n","    def ATE_absolute_error(self,concat_pred):\n","        p = self.split_pred(concat_pred)\n","        ATT_pred = tf.gather(params=self.data['y'], indices=self.data['t_idx']) - tf.gather(params=p['y0_pred'], indices=self.data['t_idx'])\n","        ATU_pred = tf.gather(params=p['y1_pred'], indices=self.data['c_idx']) - tf.gather(params=self.data['y'], indices=self.data['c_idx'])\n","        ATE_pred = tf.reduce_mean(tf.concat([ATT_pred,ATU_pred], axis=0)) #stitch em back up!\n","        ATE_actual = tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n","        return tf.abs(ATE_actual- ATE_pred)\n","\n","    def ITE_RMSE_error(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        y_1_treated_group = tf.gather(params=self.data['y'], indices=self.data['t_idx'])\n","        y_0_treated_group = tf.gather(params=p['y0_pred'], indices=self.data['t_idx'])\n","\n","        mu_1_treated_group = tf.gather(params=self.data['mu_1'], indices=self.data['t_idx'])\n","        mu_0_treated_group = tf.gather(params=self.data['mu_0'], indices=self.data['t_idx'])\n","\n","\n","        treat_grp_error = (y_1_treated_group - y_0_treated_group) - (mu_1_treated_group - mu_0_treated_group)\n","\n","        y_1_control_group = tf.gather(params=p['y1_pred'], indices=self.data['c_idx'])\n","        y_0_control_group = tf.gather(params=self.data['y'], indices=self.data['c_idx'])\n","\n","        mu_1_control_group = tf.gather(params=self.data['mu_1'], indices=self.data['c_idx'])\n","        mu_0_control_group = tf.gather(params=self.data['mu_0'], indices=self.data['c_idx'])\n","\n","        control_grp_error = (y_1_control_group - y_0_control_group) - (mu_1_control_group - mu_0_control_group)\n","\n","\n","        ITE_error = tf.concat([treat_grp_error, control_grp_error], axis=0)\n","        ITE_RMSE_error = tf.sqrt(tf.reduce_mean(tf.square(ITE_error)))\n","        return ITE_RMSE_error\n","\n","    def compute_hstar(self,y0_pred,y1_pred,t_pred,t_true,epsilons):\n","        #helper for calculating the targeted regularization cate\n","        y_pred = t_true * y1_pred + (1 - t_true) * y0_pred\n","        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n","        h_star = y_pred + epsilons * cc\n","        return h_star\n","\n","    def TARREG_ATE_absolute_error(self,concat_pred):\n","        #Final calculation of Targeted Regularization loss\n","        p = self.split_pred(concat_pred)\n","        t_pred = tf.math.sigmoid(p['t_pred'])\n","        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n","        hstar_0=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.zeros_like(p['epsilon']),p['epsilon'])\n","        hstar_1=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.ones_like(p['epsilon']),p['epsilon'])\n","        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n","        ate_tarreg_pred = tf.reduce_mean(hstar_1-hstar_0)\n","        return tf.abs(ate_true- ate_tarreg_pred)"]},{"cell_type":"code","execution_count":null,"id":"a4b86afe","metadata":{"id":"a4b86afe"},"outputs":[],"source":["\n","#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n","class Eval_metrics_test():\n","    def __init__(self,data):\n","        self.data=data #feed the callback the full dataset\n","\n","    def split_pred(self,concat_pred):\n","        preds={}\n","        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n","        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n","        preds['t_pred'] = concat_pred[:, 2]\n","        preds['epsilon'] = concat_pred[:, 3]\n","        preds['phi'] = concat_pred[:, 4:]\n","        return preds\n","\n","\n","    def PEHE(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n","        return tf.sqrt(cate_err)\n","\n","    def PEHE_tareg(self,concat_pred):\n","        #simulation only\n","        p = self.split_pred(concat_pred)\n","        t_pred = tf.math.sigmoid(p['t_pred'])\n","        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n","        hstar_0=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.zeros_like(p['epsilon']),p['epsilon'])\n","        hstar_1=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.ones_like(p['epsilon']),p['epsilon'])\n","\n","        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (hstar_1-hstar_0) ) ) )\n","        return tf.sqrt(cate_err)\n","\n","\n","    def compute_hstar(self,y0_pred,y1_pred,t_pred,t_true,epsilons):\n","        #helper for calculating the targeted regularization cate\n","        y_pred = t_true * y1_pred + (1 - t_true) * y0_pred\n","        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n","        h_star = y_pred + epsilons * cc\n","        return h_star\n","\n","    def TARREG_ATE_absolute_error(self,concat_pred):\n","        #Final calculation of Targeted Regularization loss\n","        p = self.split_pred(concat_pred)\n","        t_pred = tf.math.sigmoid(p['t_pred'])\n","        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n","        hstar_0=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.zeros_like(p['epsilon']),p['epsilon'])\n","        hstar_1=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.ones_like(p['epsilon']),p['epsilon'])\n","        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n","        print(tf.shape(hstar_1-hstar_0))\n","        ate_tarreg_pred = tf.reduce_mean(hstar_1-hstar_0)\n","        return tf.abs(ate_true- ate_tarreg_pred)"]},{"cell_type":"code","execution_count":null,"id":"48437e2b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48437e2b","executionInfo":{"status":"ok","timestamp":1695425671019,"user_tz":240,"elapsed":2445263,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"0f36364b-289a-4c39-f181-c55ff8468ad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","Epoch 1/100000\n","1/1 [==============================] - 3s 3s/step - loss: 4933.2227 - tarreg_loss: 4928.4834 - regression_loss: 2449.0789 - treatment_acc: 0.4347 - val_loss: 1332.0057 - val_tarreg_loss: 1327.2664 - val_regression_loss: 655.5977 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 2/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 4890.4194 - tarreg_loss: 4885.6802 - regression_loss: 2430.0205 - treatment_acc: 0.4333 - val_loss: 1315.3153 - val_tarreg_loss: 1310.5759 - val_regression_loss: 648.1030 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 3/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 4836.6123 - tarreg_loss: 4831.8730 - regression_loss: 2406.0166 - treatment_acc: 0.4333 - val_loss: 1297.1826 - val_tarreg_loss: 1292.4434 - val_regression_loss: 639.9438 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 4/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 4777.5503 - tarreg_loss: 4772.8110 - regression_loss: 2379.5559 - treatment_acc: 0.4333 - val_loss: 1278.8866 - val_tarreg_loss: 1274.1473 - val_regression_loss: 631.6694 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 5/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 4716.9111 - tarreg_loss: 4712.1719 - regression_loss: 2352.1646 - treatment_acc: 0.4329 - val_loss: 1261.1061 - val_tarreg_loss: 1256.3668 - val_regression_loss: 623.5543 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 6/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 4656.4302 - tarreg_loss: 4651.6909 - regression_loss: 2324.4844 - treatment_acc: 0.4319 - val_loss: 1244.0801 - val_tarreg_loss: 1239.3408 - val_regression_loss: 615.6778 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 7/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 4596.5137 - tarreg_loss: 4591.7744 - regression_loss: 2296.5820 - treatment_acc: 0.4319 - val_loss: 1227.8099 - val_tarreg_loss: 1223.0707 - val_regression_loss: 608.0226 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 8/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 4536.9854 - tarreg_loss: 4532.2461 - regression_loss: 2268.3091 - treatment_acc: 0.4315 - val_loss: 1212.2324 - val_tarreg_loss: 1207.4932 - val_regression_loss: 600.5565 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 9/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 4477.6816 - tarreg_loss: 4472.9424 - regression_loss: 2239.5864 - treatment_acc: 0.4310 - val_loss: 1197.3179 - val_tarreg_loss: 1192.5789 - val_regression_loss: 593.2776 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 10/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 4418.7725 - tarreg_loss: 4414.0332 - regression_loss: 2210.5457 - treatment_acc: 0.4315 - val_loss: 1183.1019 - val_tarreg_loss: 1178.3629 - val_regression_loss: 586.2245 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 11/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 4360.8110 - tarreg_loss: 4356.0718 - regression_loss: 2181.5400 - treatment_acc: 0.4329 - val_loss: 1169.6646 - val_tarreg_loss: 1164.9258 - val_regression_loss: 579.4640 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 12/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 4304.5977 - tarreg_loss: 4299.8584 - regression_loss: 2153.0640 - treatment_acc: 0.4329 - val_loss: 1157.0906 - val_tarreg_loss: 1152.3518 - val_regression_loss: 573.0667 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 13/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 4250.9727 - tarreg_loss: 4246.2334 - regression_loss: 2125.6377 - treatment_acc: 0.4333 - val_loss: 1145.4290 - val_tarreg_loss: 1140.6902 - val_regression_loss: 567.0851 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 14/100000\n","1/1 [==============================] - 0s 114ms/step - loss: 4200.6177 - tarreg_loss: 4195.8784 - regression_loss: 2099.7017 - treatment_acc: 0.4338 - val_loss: 1134.6709 - val_tarreg_loss: 1129.9323 - val_regression_loss: 561.5409 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 15/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 4153.9414 - tarreg_loss: 4149.2021 - regression_loss: 2075.5503 - treatment_acc: 0.4338 - val_loss: 1124.7478 - val_tarreg_loss: 1120.0092 - val_regression_loss: 556.4215 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 16/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 4111.0483 - tarreg_loss: 4106.3091 - regression_loss: 2053.3140 - treatment_acc: 0.4343 - val_loss: 1115.5479 - val_tarreg_loss: 1110.8092 - val_regression_loss: 551.6881 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 17/100000\n","1/1 [==============================] - 0s 133ms/step - loss: 4071.7830 - tarreg_loss: 4067.0444 - regression_loss: 2032.9758 - treatment_acc: 0.4347 - val_loss: 1106.9404 - val_tarreg_loss: 1102.2019 - val_regression_loss: 547.2875 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 18/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 4035.8281 - tarreg_loss: 4031.0896 - regression_loss: 2014.4187 - treatment_acc: 0.4347 - val_loss: 1098.8014 - val_tarreg_loss: 1094.0629 - val_regression_loss: 543.1638 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 19/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 4002.7893 - tarreg_loss: 3998.0510 - regression_loss: 1997.4698 - treatment_acc: 0.4352 - val_loss: 1091.0302 - val_tarreg_loss: 1086.2916 - val_regression_loss: 539.2684 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 20/100000\n","1/1 [==============================] - 0s 157ms/step - loss: 3972.2834 - tarreg_loss: 3967.5447 - regression_loss: 1981.9436 - treatment_acc: 0.4352 - val_loss: 1083.5596 - val_tarreg_loss: 1078.8210 - val_regression_loss: 535.5655 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 21/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 3943.9797 - tarreg_loss: 3939.2412 - regression_loss: 1967.6677 - treatment_acc: 0.4356 - val_loss: 1076.3549 - val_tarreg_loss: 1071.6163 - val_regression_loss: 532.0319 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 22/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3917.6194 - tarreg_loss: 3912.8811 - regression_loss: 1954.4939 - treatment_acc: 0.4356 - val_loss: 1069.4087 - val_tarreg_loss: 1064.6702 - val_regression_loss: 528.6562 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 23/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3893.0083 - tarreg_loss: 3888.2698 - regression_loss: 1942.2991 - treatment_acc: 0.4361 - val_loss: 1062.7273 - val_tarreg_loss: 1057.9890 - val_regression_loss: 525.4335 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 24/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 3869.9941 - tarreg_loss: 3865.2556 - regression_loss: 1930.9783 - treatment_acc: 0.4361 - val_loss: 1056.3265 - val_tarreg_loss: 1051.5884 - val_regression_loss: 522.3627 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 25/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3848.4463 - tarreg_loss: 3843.7078 - regression_loss: 1920.4371 - treatment_acc: 0.4356 - val_loss: 1050.2190 - val_tarreg_loss: 1045.4808 - val_regression_loss: 519.4423 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 26/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 3828.2405 - tarreg_loss: 3823.5020 - regression_loss: 1910.5864 - treatment_acc: 0.4356 - val_loss: 1044.4122 - val_tarreg_loss: 1039.6741 - val_regression_loss: 516.6696 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 27/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3809.2485 - tarreg_loss: 3804.5103 - regression_loss: 1901.3397 - treatment_acc: 0.4356 - val_loss: 1038.9070 - val_tarreg_loss: 1034.1689 - val_regression_loss: 514.0397 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 28/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 3791.3420 - tarreg_loss: 3786.6038 - regression_loss: 1892.6147 - treatment_acc: 0.4356 - val_loss: 1033.6968 - val_tarreg_loss: 1028.9587 - val_regression_loss: 511.5460 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 29/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3774.3940 - tarreg_loss: 3769.6558 - regression_loss: 1884.3362 - treatment_acc: 0.4356 - val_loss: 1028.7701 - val_tarreg_loss: 1024.0321 - val_regression_loss: 509.1807 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 30/100000\n","1/1 [==============================] - 0s 140ms/step - loss: 3758.2881 - tarreg_loss: 3753.5498 - regression_loss: 1876.4386 - treatment_acc: 0.4356 - val_loss: 1024.1123 - val_tarreg_loss: 1019.3743 - val_regression_loss: 506.9356 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 31/100000\n","1/1 [==============================] - 0s 187ms/step - loss: 3742.9224 - tarreg_loss: 3738.1841 - regression_loss: 1868.8685 - treatment_acc: 0.4361 - val_loss: 1019.7072 - val_tarreg_loss: 1014.9692 - val_regression_loss: 504.8032 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 32/100000\n","1/1 [==============================] - 0s 156ms/step - loss: 3728.2134 - tarreg_loss: 3723.4751 - regression_loss: 1861.5859 - treatment_acc: 0.4356 - val_loss: 1015.5374 - val_tarreg_loss: 1010.7994 - val_regression_loss: 502.7761 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 33/100000\n","1/1 [==============================] - 0s 160ms/step - loss: 3714.0964 - tarreg_loss: 3709.3586 - regression_loss: 1854.5626 - treatment_acc: 0.4356 - val_loss: 1011.5858 - val_tarreg_loss: 1006.8478 - val_regression_loss: 500.8474 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 34/100000\n","1/1 [==============================] - 0s 165ms/step - loss: 3700.5225 - tarreg_loss: 3695.7847 - regression_loss: 1847.7795 - treatment_acc: 0.4356 - val_loss: 1007.8350 - val_tarreg_loss: 1003.0970 - val_regression_loss: 499.0106 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 35/100000\n","1/1 [==============================] - 0s 161ms/step - loss: 3687.4526 - tarreg_loss: 3682.7148 - regression_loss: 1841.2249 - treatment_acc: 0.4356 - val_loss: 1004.2679 - val_tarreg_loss: 999.5300 - val_regression_loss: 497.2595 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 36/100000\n","1/1 [==============================] - 0s 162ms/step - loss: 3674.8572 - tarreg_loss: 3670.1194 - regression_loss: 1834.8906 - treatment_acc: 0.4356 - val_loss: 1000.8677 - val_tarreg_loss: 996.1299 - val_regression_loss: 495.5877 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 37/100000\n","1/1 [==============================] - 0s 160ms/step - loss: 3662.7092 - tarreg_loss: 3657.9714 - regression_loss: 1828.7708 - treatment_acc: 0.4356 - val_loss: 997.6188 - val_tarreg_loss: 992.8810 - val_regression_loss: 493.9893 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 38/100000\n","1/1 [==============================] - 0s 166ms/step - loss: 3650.9846 - tarreg_loss: 3646.2468 - regression_loss: 1822.8591 - treatment_acc: 0.4343 - val_loss: 994.5062 - val_tarreg_loss: 989.7685 - val_regression_loss: 492.4583 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 39/100000\n","1/1 [==============================] - 0s 154ms/step - loss: 3639.6597 - tarreg_loss: 3634.9221 - regression_loss: 1817.1493 - treatment_acc: 0.4343 - val_loss: 991.5170 - val_tarreg_loss: 986.7794 - val_regression_loss: 490.9894 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 40/100000\n","1/1 [==============================] - 0s 155ms/step - loss: 3628.7112 - tarreg_loss: 3623.9736 - regression_loss: 1811.6334 - treatment_acc: 0.4343 - val_loss: 988.6401 - val_tarreg_loss: 983.9025 - val_regression_loss: 489.5781 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 41/100000\n","1/1 [==============================] - 0s 149ms/step - loss: 3618.1184 - tarreg_loss: 3613.3809 - regression_loss: 1806.3037 - treatment_acc: 0.4343 - val_loss: 985.8670 - val_tarreg_loss: 981.1293 - val_regression_loss: 488.2203 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 42/100000\n","1/1 [==============================] - 0s 166ms/step - loss: 3607.8586 - tarreg_loss: 3603.1211 - regression_loss: 1801.1505 - treatment_acc: 0.4343 - val_loss: 983.1910 - val_tarreg_loss: 978.4533 - val_regression_loss: 486.9132 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 43/100000\n","1/1 [==============================] - 0s 185ms/step - loss: 3597.9148 - tarreg_loss: 3593.1772 - regression_loss: 1796.1650 - treatment_acc: 0.4347 - val_loss: 980.6077 - val_tarreg_loss: 975.8701 - val_regression_loss: 485.6541 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 44/100000\n","1/1 [==============================] - 0s 183ms/step - loss: 3588.2686 - tarreg_loss: 3583.5310 - regression_loss: 1791.3381 - treatment_acc: 0.4352 - val_loss: 978.1148 - val_tarreg_loss: 973.3773 - val_regression_loss: 484.4418 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 45/100000\n","1/1 [==============================] - 0s 208ms/step - loss: 3578.9045 - tarreg_loss: 3574.1670 - regression_loss: 1786.6609 - treatment_acc: 0.4347 - val_loss: 975.7106 - val_tarreg_loss: 970.9731 - val_regression_loss: 483.2748 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 46/100000\n","1/1 [==============================] - 0s 181ms/step - loss: 3569.8086 - tarreg_loss: 3565.0710 - regression_loss: 1782.1244 - treatment_acc: 0.4347 - val_loss: 973.3947 - val_tarreg_loss: 968.6571 - val_regression_loss: 482.1525 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 47/100000\n","1/1 [==============================] - 0s 198ms/step - loss: 3560.9670 - tarreg_loss: 3556.2295 - regression_loss: 1777.7203 - treatment_acc: 0.4347 - val_loss: 971.1668 - val_tarreg_loss: 966.4293 - val_regression_loss: 481.0743 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 48/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3552.3674 - tarreg_loss: 3547.6301 - regression_loss: 1773.4406 - treatment_acc: 0.4347 - val_loss: 969.0264 - val_tarreg_loss: 964.2889 - val_regression_loss: 480.0394 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 49/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 3543.9976 - tarreg_loss: 3539.2603 - regression_loss: 1769.2771 - treatment_acc: 0.4352 - val_loss: 966.9730 - val_tarreg_loss: 962.2355 - val_regression_loss: 479.0472 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 50/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 3535.8450 - tarreg_loss: 3531.1077 - regression_loss: 1765.2223 - treatment_acc: 0.4356 - val_loss: 965.0056 - val_tarreg_loss: 960.2681 - val_regression_loss: 478.0968 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 51/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3527.8984 - tarreg_loss: 3523.1611 - regression_loss: 1761.2695 - treatment_acc: 0.4356 - val_loss: 963.1223 - val_tarreg_loss: 958.3849 - val_regression_loss: 477.1872 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 52/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3520.1467 - tarreg_loss: 3515.4094 - regression_loss: 1757.4121 - treatment_acc: 0.4356 - val_loss: 961.3210 - val_tarreg_loss: 956.5836 - val_regression_loss: 476.3170 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 53/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3512.5781 - tarreg_loss: 3507.8408 - regression_loss: 1753.6439 - treatment_acc: 0.4352 - val_loss: 959.5984 - val_tarreg_loss: 954.8611 - val_regression_loss: 475.4846 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 54/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 3505.1831 - tarreg_loss: 3500.4458 - regression_loss: 1749.9600 - treatment_acc: 0.4356 - val_loss: 957.9514 - val_tarreg_loss: 953.2140 - val_regression_loss: 474.6884 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 55/100000\n","1/1 [==============================] - 0s 110ms/step - loss: 3497.9526 - tarreg_loss: 3493.2153 - regression_loss: 1746.3553 - treatment_acc: 0.4352 - val_loss: 956.3757 - val_tarreg_loss: 951.6384 - val_regression_loss: 473.9265 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 56/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 3490.8772 - tarreg_loss: 3486.1399 - regression_loss: 1742.8259 - treatment_acc: 0.4352 - val_loss: 954.8676 - val_tarreg_loss: 950.1302 - val_regression_loss: 473.1971 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 57/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3483.9495 - tarreg_loss: 3479.2122 - regression_loss: 1739.3682 - treatment_acc: 0.4352 - val_loss: 953.4225 - val_tarreg_loss: 948.6852 - val_regression_loss: 472.4980 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 58/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 3477.1619 - tarreg_loss: 3472.4246 - regression_loss: 1735.9786 - treatment_acc: 0.4352 - val_loss: 952.0364 - val_tarreg_loss: 947.2992 - val_regression_loss: 471.8274 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 59/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3470.5066 - tarreg_loss: 3465.7693 - regression_loss: 1732.6539 - treatment_acc: 0.4352 - val_loss: 950.7051 - val_tarreg_loss: 945.9679 - val_regression_loss: 471.1833 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 60/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3463.9788 - tarreg_loss: 3459.2415 - regression_loss: 1729.3920 - treatment_acc: 0.4352 - val_loss: 949.4245 - val_tarreg_loss: 944.6873 - val_regression_loss: 470.5638 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 61/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3457.5713 - tarreg_loss: 3452.8342 - regression_loss: 1726.1899 - treatment_acc: 0.4347 - val_loss: 948.1910 - val_tarreg_loss: 943.4538 - val_regression_loss: 469.9673 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 62/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3451.2793 - tarreg_loss: 3446.5422 - regression_loss: 1723.0454 - treatment_acc: 0.4347 - val_loss: 947.0010 - val_tarreg_loss: 942.2638 - val_regression_loss: 469.3921 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 63/100000\n","1/1 [==============================] - 0s 162ms/step - loss: 3445.0977 - tarreg_loss: 3440.3606 - regression_loss: 1719.9563 - treatment_acc: 0.4347 - val_loss: 945.8516 - val_tarreg_loss: 941.1144 - val_regression_loss: 468.8367 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 64/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3439.0215 - tarreg_loss: 3434.2844 - regression_loss: 1716.9198 - treatment_acc: 0.4347 - val_loss: 944.7401 - val_tarreg_loss: 940.0029 - val_regression_loss: 468.3000 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 65/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3433.0461 - tarreg_loss: 3428.3091 - regression_loss: 1713.9343 - treatment_acc: 0.4347 - val_loss: 943.6643 - val_tarreg_loss: 938.9272 - val_regression_loss: 467.7807 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 66/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 3427.1677 - tarreg_loss: 3422.4307 - regression_loss: 1710.9977 - treatment_acc: 0.4347 - val_loss: 942.6223 - val_tarreg_loss: 937.8851 - val_regression_loss: 467.2780 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 67/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3421.3816 - tarreg_loss: 3416.6445 - regression_loss: 1708.1078 - treatment_acc: 0.4347 - val_loss: 941.6121 - val_tarreg_loss: 936.8750 - val_regression_loss: 466.7908 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 68/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3415.6853 - tarreg_loss: 3410.9482 - regression_loss: 1705.2627 - treatment_acc: 0.4338 - val_loss: 940.6324 - val_tarreg_loss: 935.8953 - val_regression_loss: 466.3185 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 69/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3410.0745 - tarreg_loss: 3405.3374 - regression_loss: 1702.4607 - treatment_acc: 0.4338 - val_loss: 939.6822 - val_tarreg_loss: 934.9451 - val_regression_loss: 465.8604 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 70/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 3404.5461 - tarreg_loss: 3399.8091 - regression_loss: 1699.7003 - treatment_acc: 0.4343 - val_loss: 938.7600 - val_tarreg_loss: 934.0229 - val_regression_loss: 465.4160 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 71/100000\n","1/1 [==============================] - 0s 139ms/step - loss: 3399.0977 - tarreg_loss: 3394.3606 - regression_loss: 1696.9795 - treatment_acc: 0.4352 - val_loss: 937.8649 - val_tarreg_loss: 933.1278 - val_regression_loss: 464.9846 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 72/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3393.7261 - tarreg_loss: 3388.9890 - regression_loss: 1694.2970 - treatment_acc: 0.4356 - val_loss: 936.9959 - val_tarreg_loss: 932.2589 - val_regression_loss: 464.5659 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 73/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3388.4282 - tarreg_loss: 3383.6912 - regression_loss: 1691.6512 - treatment_acc: 0.4361 - val_loss: 936.1521 - val_tarreg_loss: 931.4150 - val_regression_loss: 464.1592 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 74/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3383.2012 - tarreg_loss: 3378.4641 - regression_loss: 1689.0409 - treatment_acc: 0.4361 - val_loss: 935.3325 - val_tarreg_loss: 930.5954 - val_regression_loss: 463.7642 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 75/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 3378.0437 - tarreg_loss: 3373.3066 - regression_loss: 1686.4646 - treatment_acc: 0.4356 - val_loss: 934.5360 - val_tarreg_loss: 929.7990 - val_regression_loss: 463.3801 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 76/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 3372.9531 - tarreg_loss: 3368.2161 - regression_loss: 1683.9216 - treatment_acc: 0.4356 - val_loss: 933.7617 - val_tarreg_loss: 929.0247 - val_regression_loss: 463.0067 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 77/100000\n","1/1 [==============================] - 0s 114ms/step - loss: 3367.9265 - tarreg_loss: 3363.1895 - regression_loss: 1681.4102 - treatment_acc: 0.4352 - val_loss: 933.0085 - val_tarreg_loss: 928.2715 - val_regression_loss: 462.6433 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 78/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3362.9626 - tarreg_loss: 3358.2256 - regression_loss: 1678.9294 - treatment_acc: 0.4352 - val_loss: 932.2754 - val_tarreg_loss: 927.5384 - val_regression_loss: 462.2896 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 79/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3358.0583 - tarreg_loss: 3353.3213 - regression_loss: 1676.4786 - treatment_acc: 0.4356 - val_loss: 931.5613 - val_tarreg_loss: 926.8243 - val_regression_loss: 461.9449 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 80/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 3353.2131 - tarreg_loss: 3348.4761 - regression_loss: 1674.0568 - treatment_acc: 0.4361 - val_loss: 930.8655 - val_tarreg_loss: 926.1285 - val_regression_loss: 461.6089 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 81/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3348.4236 - tarreg_loss: 3343.6865 - regression_loss: 1671.6626 - treatment_acc: 0.4361 - val_loss: 930.1868 - val_tarreg_loss: 925.4498 - val_regression_loss: 461.2809 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 82/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 3343.6890 - tarreg_loss: 3338.9519 - regression_loss: 1669.2957 - treatment_acc: 0.4361 - val_loss: 929.5240 - val_tarreg_loss: 924.7871 - val_regression_loss: 460.9606 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 83/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3339.0071 - tarreg_loss: 3334.2700 - regression_loss: 1666.9547 - treatment_acc: 0.4356 - val_loss: 928.8767 - val_tarreg_loss: 924.1398 - val_regression_loss: 460.6477 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 84/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 3334.3762 - tarreg_loss: 3329.6392 - regression_loss: 1664.6395 - treatment_acc: 0.4356 - val_loss: 928.2436 - val_tarreg_loss: 923.5067 - val_regression_loss: 460.3415 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 85/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3329.7957 - tarreg_loss: 3325.0586 - regression_loss: 1662.3492 - treatment_acc: 0.4361 - val_loss: 927.6243 - val_tarreg_loss: 922.8874 - val_regression_loss: 460.0419 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 86/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3325.2632 - tarreg_loss: 3320.5261 - regression_loss: 1660.0828 - treatment_acc: 0.4356 - val_loss: 927.0179 - val_tarreg_loss: 922.2811 - val_regression_loss: 459.7485 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 87/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3320.7773 - tarreg_loss: 3316.0403 - regression_loss: 1657.8396 - treatment_acc: 0.4356 - val_loss: 926.4239 - val_tarreg_loss: 921.6870 - val_regression_loss: 459.4609 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 88/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 3316.3372 - tarreg_loss: 3311.6001 - regression_loss: 1655.6194 - treatment_acc: 0.4356 - val_loss: 925.8414 - val_tarreg_loss: 921.1046 - val_regression_loss: 459.1788 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 89/100000\n","1/1 [==============================] - 0s 130ms/step - loss: 3311.9412 - tarreg_loss: 3307.2041 - regression_loss: 1653.4211 - treatment_acc: 0.4356 - val_loss: 925.2703 - val_tarreg_loss: 920.5334 - val_regression_loss: 458.9021 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 90/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3307.5876 - tarreg_loss: 3302.8506 - regression_loss: 1651.2440 - treatment_acc: 0.4356 - val_loss: 924.7100 - val_tarreg_loss: 919.9731 - val_regression_loss: 458.6306 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 91/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3303.2761 - tarreg_loss: 3298.5391 - regression_loss: 1649.0881 - treatment_acc: 0.4361 - val_loss: 924.1602 - val_tarreg_loss: 919.4233 - val_regression_loss: 458.3640 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 92/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3299.0051 - tarreg_loss: 3294.2681 - regression_loss: 1646.9524 - treatment_acc: 0.4361 - val_loss: 923.6202 - val_tarreg_loss: 918.8833 - val_regression_loss: 458.1021 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 93/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3294.7734 - tarreg_loss: 3290.0364 - regression_loss: 1644.8362 - treatment_acc: 0.4361 - val_loss: 923.0900 - val_tarreg_loss: 918.3531 - val_regression_loss: 457.8448 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 94/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3290.5801 - tarreg_loss: 3285.8430 - regression_loss: 1642.7393 - treatment_acc: 0.4370 - val_loss: 922.5693 - val_tarreg_loss: 917.8324 - val_regression_loss: 457.5921 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 95/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3286.4238 - tarreg_loss: 3281.6868 - regression_loss: 1640.6609 - treatment_acc: 0.4370 - val_loss: 922.0577 - val_tarreg_loss: 917.3209 - val_regression_loss: 457.3437 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 96/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3282.3047 - tarreg_loss: 3277.5676 - regression_loss: 1638.6008 - treatment_acc: 0.4370 - val_loss: 921.5549 - val_tarreg_loss: 916.8181 - val_regression_loss: 457.0994 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 97/100000\n","1/1 [==============================] - 0s 133ms/step - loss: 3278.2200 - tarreg_loss: 3273.4829 - regression_loss: 1636.5581 - treatment_acc: 0.4370 - val_loss: 921.0610 - val_tarreg_loss: 916.3241 - val_regression_loss: 456.8593 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 98/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3274.1707 - tarreg_loss: 3269.4336 - regression_loss: 1634.5330 - treatment_acc: 0.4370 - val_loss: 920.5753 - val_tarreg_loss: 915.8384 - val_regression_loss: 456.6231 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 99/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3270.1548 - tarreg_loss: 3265.4177 - regression_loss: 1632.5244 - treatment_acc: 0.4370 - val_loss: 920.0978 - val_tarreg_loss: 915.3609 - val_regression_loss: 456.3908 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 100/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 3266.1716 - tarreg_loss: 3261.4346 - regression_loss: 1630.5323 - treatment_acc: 0.4361 - val_loss: 919.6284 - val_tarreg_loss: 914.8915 - val_regression_loss: 456.1623 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 101/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3262.2205 - tarreg_loss: 3257.4834 - regression_loss: 1628.5560 - treatment_acc: 0.4361 - val_loss: 919.1667 - val_tarreg_loss: 914.4299 - val_regression_loss: 455.9375 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 102/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3258.3010 - tarreg_loss: 3253.5640 - regression_loss: 1626.5956 - treatment_acc: 0.4361 - val_loss: 918.7126 - val_tarreg_loss: 913.9757 - val_regression_loss: 455.7162 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 103/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3254.4119 - tarreg_loss: 3249.6748 - regression_loss: 1624.6504 - treatment_acc: 0.4366 - val_loss: 918.2658 - val_tarreg_loss: 913.5289 - val_regression_loss: 455.4985 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 104/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3250.5525 - tarreg_loss: 3245.8154 - regression_loss: 1622.7200 - treatment_acc: 0.4366 - val_loss: 917.8263 - val_tarreg_loss: 913.0894 - val_regression_loss: 455.2842 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 105/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3246.7222 - tarreg_loss: 3241.9854 - regression_loss: 1620.8042 - treatment_acc: 0.4370 - val_loss: 917.3937 - val_tarreg_loss: 912.6568 - val_regression_loss: 455.0732 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 106/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3242.9207 - tarreg_loss: 3238.1841 - regression_loss: 1618.9028 - treatment_acc: 0.4375 - val_loss: 916.9678 - val_tarreg_loss: 912.2310 - val_regression_loss: 454.8654 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 107/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 3239.1467 - tarreg_loss: 3234.4102 - regression_loss: 1617.0151 - treatment_acc: 0.4380 - val_loss: 916.5488 - val_tarreg_loss: 911.8119 - val_regression_loss: 454.6608 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 108/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3235.4006 - tarreg_loss: 3230.6641 - regression_loss: 1615.1411 - treatment_acc: 0.4380 - val_loss: 916.1360 - val_tarreg_loss: 911.3992 - val_regression_loss: 454.4593 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 109/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3231.6816 - tarreg_loss: 3226.9451 - regression_loss: 1613.2810 - treatment_acc: 0.4380 - val_loss: 915.7296 - val_tarreg_loss: 910.9927 - val_regression_loss: 454.2607 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 110/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3227.9880 - tarreg_loss: 3223.2515 - regression_loss: 1611.4333 - treatment_acc: 0.4380 - val_loss: 915.3293 - val_tarreg_loss: 910.5924 - val_regression_loss: 454.0651 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 111/100000\n","1/1 [==============================] - 0s 114ms/step - loss: 3224.3206 - tarreg_loss: 3219.5840 - regression_loss: 1609.5988 - treatment_acc: 0.4380 - val_loss: 914.9351 - val_tarreg_loss: 910.1982 - val_regression_loss: 453.8724 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 112/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3220.6780 - tarreg_loss: 3215.9414 - regression_loss: 1607.7767 - treatment_acc: 0.4380 - val_loss: 914.5466 - val_tarreg_loss: 909.8098 - val_regression_loss: 453.6825 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 113/100000\n","1/1 [==============================] - 0s 145ms/step - loss: 3217.0598 - tarreg_loss: 3212.3232 - regression_loss: 1605.9670 - treatment_acc: 0.4380 - val_loss: 914.1639 - val_tarreg_loss: 909.4271 - val_regression_loss: 453.4953 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 114/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 3213.4661 - tarreg_loss: 3208.7295 - regression_loss: 1604.1693 - treatment_acc: 0.4380 - val_loss: 913.7869 - val_tarreg_loss: 909.0500 - val_regression_loss: 453.3108 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 115/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 3209.8960 - tarreg_loss: 3205.1594 - regression_loss: 1602.3835 - treatment_acc: 0.4375 - val_loss: 913.4154 - val_tarreg_loss: 908.6785 - val_regression_loss: 453.1290 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 116/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 3206.3489 - tarreg_loss: 3201.6123 - regression_loss: 1600.6093 - treatment_acc: 0.4375 - val_loss: 913.0492 - val_tarreg_loss: 908.3123 - val_regression_loss: 452.9496 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 117/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3202.8245 - tarreg_loss: 3198.0879 - regression_loss: 1598.8464 - treatment_acc: 0.4375 - val_loss: 912.6885 - val_tarreg_loss: 907.9516 - val_regression_loss: 452.7730 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 118/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3199.3225 - tarreg_loss: 3194.5859 - regression_loss: 1597.0947 - treatment_acc: 0.4375 - val_loss: 912.3328 - val_tarreg_loss: 907.5959 - val_regression_loss: 452.5988 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 119/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3195.8420 - tarreg_loss: 3191.1055 - regression_loss: 1595.3538 - treatment_acc: 0.4380 - val_loss: 911.9822 - val_tarreg_loss: 907.2454 - val_regression_loss: 452.4269 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 120/100000\n","1/1 [==============================] - 0s 169ms/step - loss: 3192.3835 - tarreg_loss: 3187.6470 - regression_loss: 1593.6238 - treatment_acc: 0.4384 - val_loss: 911.6367 - val_tarreg_loss: 906.8998 - val_regression_loss: 452.2576 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 121/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3188.9456 - tarreg_loss: 3184.2090 - regression_loss: 1591.9043 - treatment_acc: 0.4384 - val_loss: 911.2962 - val_tarreg_loss: 906.5593 - val_regression_loss: 452.0906 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 122/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3185.5286 - tarreg_loss: 3180.7920 - regression_loss: 1590.1951 - treatment_acc: 0.4384 - val_loss: 910.9604 - val_tarreg_loss: 906.2235 - val_regression_loss: 451.9260 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 123/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 3182.1321 - tarreg_loss: 3177.3955 - regression_loss: 1588.4961 - treatment_acc: 0.4384 - val_loss: 910.6296 - val_tarreg_loss: 905.8927 - val_regression_loss: 451.7637 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 124/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3178.7554 - tarreg_loss: 3174.0188 - regression_loss: 1586.8071 - treatment_acc: 0.4384 - val_loss: 910.3033 - val_tarreg_loss: 905.5664 - val_regression_loss: 451.6036 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 125/100000\n","1/1 [==============================] - 0s 114ms/step - loss: 3175.3979 - tarreg_loss: 3170.6614 - regression_loss: 1585.1278 - treatment_acc: 0.4375 - val_loss: 909.9819 - val_tarreg_loss: 905.2449 - val_regression_loss: 451.4458 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 126/100000\n","1/1 [==============================] - 0s 174ms/step - loss: 3172.0598 - tarreg_loss: 3167.3232 - regression_loss: 1583.4579 - treatment_acc: 0.4380 - val_loss: 909.6648 - val_tarreg_loss: 904.9279 - val_regression_loss: 451.2902 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 127/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 3168.7407 - tarreg_loss: 3164.0042 - regression_loss: 1581.7979 - treatment_acc: 0.4380 - val_loss: 909.3522 - val_tarreg_loss: 904.6153 - val_regression_loss: 451.1367 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 128/100000\n","1/1 [==============================] - 0s 179ms/step - loss: 3165.4397 - tarreg_loss: 3160.7031 - regression_loss: 1580.1467 - treatment_acc: 0.4380 - val_loss: 909.0441 - val_tarreg_loss: 904.3071 - val_regression_loss: 450.9853 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 129/100000\n","1/1 [==============================] - 0s 227ms/step - loss: 3162.1572 - tarreg_loss: 3157.4207 - regression_loss: 1578.5049 - treatment_acc: 0.4380 - val_loss: 908.7403 - val_tarreg_loss: 904.0034 - val_regression_loss: 450.8361 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 130/100000\n","1/1 [==============================] - 0s 198ms/step - loss: 3158.8926 - tarreg_loss: 3154.1560 - regression_loss: 1576.8718 - treatment_acc: 0.4380 - val_loss: 908.4405 - val_tarreg_loss: 903.7036 - val_regression_loss: 450.6888 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 131/100000\n","1/1 [==============================] - 0s 201ms/step - loss: 3155.6460 - tarreg_loss: 3150.9094 - regression_loss: 1575.2478 - treatment_acc: 0.4375 - val_loss: 908.1449 - val_tarreg_loss: 903.4080 - val_regression_loss: 450.5435 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 132/100000\n","1/1 [==============================] - 0s 196ms/step - loss: 3152.4163 - tarreg_loss: 3147.6797 - regression_loss: 1573.6323 - treatment_acc: 0.4375 - val_loss: 907.8534 - val_tarreg_loss: 903.1165 - val_regression_loss: 450.4001 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 133/100000\n","1/1 [==============================] - 0s 199ms/step - loss: 3149.2036 - tarreg_loss: 3144.4670 - regression_loss: 1572.0254 - treatment_acc: 0.4370 - val_loss: 907.5659 - val_tarreg_loss: 902.8289 - val_regression_loss: 450.2587 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 134/100000\n","1/1 [==============================] - 0s 180ms/step - loss: 3146.0078 - tarreg_loss: 3141.2712 - regression_loss: 1570.4268 - treatment_acc: 0.4375 - val_loss: 907.2823 - val_tarreg_loss: 902.5453 - val_regression_loss: 450.1193 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 135/100000\n","1/1 [==============================] - 0s 163ms/step - loss: 3142.8281 - tarreg_loss: 3138.0916 - regression_loss: 1568.8364 - treatment_acc: 0.4380 - val_loss: 907.0026 - val_tarreg_loss: 902.2656 - val_regression_loss: 449.9817 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 136/100000\n","1/1 [==============================] - 0s 138ms/step - loss: 3139.6650 - tarreg_loss: 3134.9285 - regression_loss: 1567.2542 - treatment_acc: 0.4375 - val_loss: 906.7266 - val_tarreg_loss: 901.9896 - val_regression_loss: 449.8458 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 137/100000\n","1/1 [==============================] - 0s 171ms/step - loss: 3136.5176 - tarreg_loss: 3131.7810 - regression_loss: 1565.6797 - treatment_acc: 0.4375 - val_loss: 906.4543 - val_tarreg_loss: 901.7173 - val_regression_loss: 449.7119 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 138/100000\n","1/1 [==============================] - 0s 188ms/step - loss: 3133.3857 - tarreg_loss: 3128.6492 - regression_loss: 1564.1134 - treatment_acc: 0.4370 - val_loss: 906.1855 - val_tarreg_loss: 901.4485 - val_regression_loss: 449.5796 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 139/100000\n","1/1 [==============================] - 0s 236ms/step - loss: 3130.2695 - tarreg_loss: 3125.5330 - regression_loss: 1562.5546 - treatment_acc: 0.4375 - val_loss: 905.9205 - val_tarreg_loss: 901.1835 - val_regression_loss: 449.4491 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 140/100000\n","1/1 [==============================] - 0s 178ms/step - loss: 3127.1685 - tarreg_loss: 3122.4319 - regression_loss: 1561.0034 - treatment_acc: 0.4375 - val_loss: 905.6590 - val_tarreg_loss: 900.9220 - val_regression_loss: 449.3204 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 141/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 3124.0825 - tarreg_loss: 3119.3459 - regression_loss: 1559.4597 - treatment_acc: 0.4380 - val_loss: 905.4009 - val_tarreg_loss: 900.6639 - val_regression_loss: 449.1933 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 142/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3121.0112 - tarreg_loss: 3116.2747 - regression_loss: 1557.9235 - treatment_acc: 0.4380 - val_loss: 905.1464 - val_tarreg_loss: 900.4094 - val_regression_loss: 449.0679 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 143/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3117.9543 - tarreg_loss: 3113.2178 - regression_loss: 1556.3945 - treatment_acc: 0.4380 - val_loss: 904.8952 - val_tarreg_loss: 900.1582 - val_regression_loss: 448.9442 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 144/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3114.9116 - tarreg_loss: 3110.1750 - regression_loss: 1554.8726 - treatment_acc: 0.4380 - val_loss: 904.6474 - val_tarreg_loss: 899.9104 - val_regression_loss: 448.8221 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 145/100000\n","1/1 [==============================] - 0s 141ms/step - loss: 3111.8831 - tarreg_loss: 3107.1465 - regression_loss: 1553.3577 - treatment_acc: 0.4380 - val_loss: 904.4028 - val_tarreg_loss: 899.6658 - val_regression_loss: 448.7016 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 146/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 3108.8682 - tarreg_loss: 3104.1316 - regression_loss: 1551.8496 - treatment_acc: 0.4380 - val_loss: 904.1613 - val_tarreg_loss: 899.4243 - val_regression_loss: 448.5826 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 147/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 3105.8672 - tarreg_loss: 3101.1306 - regression_loss: 1550.3485 - treatment_acc: 0.4380 - val_loss: 903.9233 - val_tarreg_loss: 899.1863 - val_regression_loss: 448.4653 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 148/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3102.8794 - tarreg_loss: 3098.1428 - regression_loss: 1548.8540 - treatment_acc: 0.4380 - val_loss: 903.6884 - val_tarreg_loss: 898.9513 - val_regression_loss: 448.3495 - val_treatment_acc: 0.4519 - lr: 1.0000e-07\n","Epoch 149/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3099.9050 - tarreg_loss: 3095.1685 - regression_loss: 1547.3662 - treatment_acc: 0.4380 - val_loss: 903.4564 - val_tarreg_loss: 898.7194 - val_regression_loss: 448.2352 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 150/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3096.9436 - tarreg_loss: 3092.2070 - regression_loss: 1545.8849 - treatment_acc: 0.4380 - val_loss: 903.2275 - val_tarreg_loss: 898.4905 - val_regression_loss: 448.1223 - val_treatment_acc: 0.4537 - lr: 1.0000e-07\n","Epoch 151/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 3093.9949 - tarreg_loss: 3089.2583 - regression_loss: 1544.4099 - treatment_acc: 0.4380 - val_loss: 903.0016 - val_tarreg_loss: 898.2646 - val_regression_loss: 448.0110 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 152/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3091.0591 - tarreg_loss: 3086.3223 - regression_loss: 1542.9413 - treatment_acc: 0.4380 - val_loss: 902.7787 - val_tarreg_loss: 898.0416 - val_regression_loss: 447.9010 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 153/100000\n","1/1 [==============================] - 0s 111ms/step - loss: 3088.1357 - tarreg_loss: 3083.3989 - regression_loss: 1541.4789 - treatment_acc: 0.4380 - val_loss: 902.5590 - val_tarreg_loss: 897.8219 - val_regression_loss: 447.7926 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 154/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3085.2246 - tarreg_loss: 3080.4878 - regression_loss: 1540.0228 - treatment_acc: 0.4370 - val_loss: 902.3419 - val_tarreg_loss: 897.6048 - val_regression_loss: 447.6856 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 155/100000\n","1/1 [==============================] - 0s 108ms/step - loss: 3082.3252 - tarreg_loss: 3077.5884 - regression_loss: 1538.5725 - treatment_acc: 0.4370 - val_loss: 902.1276 - val_tarreg_loss: 897.3906 - val_regression_loss: 447.5800 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 156/100000\n","1/1 [==============================] - 0s 109ms/step - loss: 3079.4380 - tarreg_loss: 3074.7012 - regression_loss: 1537.1284 - treatment_acc: 0.4370 - val_loss: 901.9162 - val_tarreg_loss: 897.1791 - val_regression_loss: 447.4756 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 157/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3076.5627 - tarreg_loss: 3071.8259 - regression_loss: 1535.6902 - treatment_acc: 0.4370 - val_loss: 901.7076 - val_tarreg_loss: 896.9706 - val_regression_loss: 447.3727 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 158/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3073.6990 - tarreg_loss: 3068.9622 - regression_loss: 1534.2577 - treatment_acc: 0.4370 - val_loss: 901.5018 - val_tarreg_loss: 896.7648 - val_regression_loss: 447.2712 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 159/100000\n","1/1 [==============================] - 0s 104ms/step - loss: 3070.8467 - tarreg_loss: 3066.1099 - regression_loss: 1532.8311 - treatment_acc: 0.4375 - val_loss: 901.2986 - val_tarreg_loss: 896.5615 - val_regression_loss: 447.1710 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 160/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3068.0056 - tarreg_loss: 3063.2688 - regression_loss: 1531.4099 - treatment_acc: 0.4375 - val_loss: 901.0983 - val_tarreg_loss: 896.3611 - val_regression_loss: 447.0721 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 161/100000\n","1/1 [==============================] - 0s 108ms/step - loss: 3065.1758 - tarreg_loss: 3060.4390 - regression_loss: 1529.9944 - treatment_acc: 0.4375 - val_loss: 900.9005 - val_tarreg_loss: 896.1634 - val_regression_loss: 446.9745 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 162/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 3062.3567 - tarreg_loss: 3057.6199 - regression_loss: 1528.5844 - treatment_acc: 0.4375 - val_loss: 900.7053 - val_tarreg_loss: 895.9682 - val_regression_loss: 446.8782 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 163/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3059.5486 - tarreg_loss: 3054.8118 - regression_loss: 1527.1797 - treatment_acc: 0.4380 - val_loss: 900.5128 - val_tarreg_loss: 895.7756 - val_regression_loss: 446.7832 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 164/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3056.7512 - tarreg_loss: 3052.0144 - regression_loss: 1525.7804 - treatment_acc: 0.4380 - val_loss: 900.3229 - val_tarreg_loss: 895.5856 - val_regression_loss: 446.6895 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 165/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3053.9644 - tarreg_loss: 3049.2275 - regression_loss: 1524.3864 - treatment_acc: 0.4380 - val_loss: 900.1355 - val_tarreg_loss: 895.3983 - val_regression_loss: 446.5970 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 166/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3051.1875 - tarreg_loss: 3046.4507 - regression_loss: 1522.9974 - treatment_acc: 0.4380 - val_loss: 899.9506 - val_tarreg_loss: 895.2134 - val_regression_loss: 446.5058 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 167/100000\n","1/1 [==============================] - 0s 111ms/step - loss: 3048.4211 - tarreg_loss: 3043.6843 - regression_loss: 1521.6136 - treatment_acc: 0.4384 - val_loss: 899.7682 - val_tarreg_loss: 895.0309 - val_regression_loss: 446.4157 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 168/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3045.6648 - tarreg_loss: 3040.9280 - regression_loss: 1520.2349 - treatment_acc: 0.4384 - val_loss: 899.5881 - val_tarreg_loss: 894.8509 - val_regression_loss: 446.3269 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 169/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 3042.9185 - tarreg_loss: 3038.1816 - regression_loss: 1518.8611 - treatment_acc: 0.4384 - val_loss: 899.4105 - val_tarreg_loss: 894.6733 - val_regression_loss: 446.2393 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 170/100000\n","1/1 [==============================] - 0s 132ms/step - loss: 3040.1819 - tarreg_loss: 3035.4451 - regression_loss: 1517.4922 - treatment_acc: 0.4384 - val_loss: 899.2353 - val_tarreg_loss: 894.4980 - val_regression_loss: 446.1528 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 171/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3037.4551 - tarreg_loss: 3032.7183 - regression_loss: 1516.1283 - treatment_acc: 0.4384 - val_loss: 899.0624 - val_tarreg_loss: 894.3252 - val_regression_loss: 446.0675 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 172/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3034.7378 - tarreg_loss: 3030.0010 - regression_loss: 1514.7690 - treatment_acc: 0.4384 - val_loss: 898.8917 - val_tarreg_loss: 894.1545 - val_regression_loss: 445.9833 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 173/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3032.0295 - tarreg_loss: 3027.2927 - regression_loss: 1513.4146 - treatment_acc: 0.4384 - val_loss: 898.7234 - val_tarreg_loss: 893.9861 - val_regression_loss: 445.9003 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 174/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 3029.3313 - tarreg_loss: 3024.5945 - regression_loss: 1512.0649 - treatment_acc: 0.4389 - val_loss: 898.5574 - val_tarreg_loss: 893.8201 - val_regression_loss: 445.8183 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 175/100000\n","1/1 [==============================] - 0s 142ms/step - loss: 3026.6421 - tarreg_loss: 3021.9050 - regression_loss: 1510.7195 - treatment_acc: 0.4394 - val_loss: 898.3934 - val_tarreg_loss: 893.6562 - val_regression_loss: 445.7375 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 176/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3023.9619 - tarreg_loss: 3019.2249 - regression_loss: 1509.3789 - treatment_acc: 0.4398 - val_loss: 898.2319 - val_tarreg_loss: 893.4947 - val_regression_loss: 445.6578 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 177/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 3021.2908 - tarreg_loss: 3016.5537 - regression_loss: 1508.0427 - treatment_acc: 0.4398 - val_loss: 898.0724 - val_tarreg_loss: 893.3352 - val_regression_loss: 445.5791 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 178/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 3018.6287 - tarreg_loss: 3013.8916 - regression_loss: 1506.7112 - treatment_acc: 0.4389 - val_loss: 897.9152 - val_tarreg_loss: 893.1779 - val_regression_loss: 445.5015 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 179/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 3015.9751 - tarreg_loss: 3011.2380 - regression_loss: 1505.3838 - treatment_acc: 0.4389 - val_loss: 897.7600 - val_tarreg_loss: 893.0228 - val_regression_loss: 445.4250 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 180/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 3013.3301 - tarreg_loss: 3008.5930 - regression_loss: 1504.0608 - treatment_acc: 0.4389 - val_loss: 897.6071 - val_tarreg_loss: 892.8699 - val_regression_loss: 445.3496 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 181/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 3010.6936 - tarreg_loss: 3005.9565 - regression_loss: 1502.7422 - treatment_acc: 0.4389 - val_loss: 897.4562 - val_tarreg_loss: 892.7189 - val_regression_loss: 445.2752 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 182/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 3008.0657 - tarreg_loss: 3003.3286 - regression_loss: 1501.4275 - treatment_acc: 0.4389 - val_loss: 897.3075 - val_tarreg_loss: 892.5703 - val_regression_loss: 445.2018 - val_treatment_acc: 0.4556 - lr: 1.0000e-07\n","Epoch 183/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 3005.4465 - tarreg_loss: 3000.7095 - regression_loss: 1500.1174 - treatment_acc: 0.4394 - val_loss: 897.1606 - val_tarreg_loss: 892.4234 - val_regression_loss: 445.1294 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 184/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 3002.8352 - tarreg_loss: 2998.0981 - regression_loss: 1498.8112 - treatment_acc: 0.4394 - val_loss: 897.0161 - val_tarreg_loss: 892.2787 - val_regression_loss: 445.0581 - val_treatment_acc: 0.4574 - lr: 1.0000e-07\n","Epoch 185/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 3000.2319 - tarreg_loss: 2995.4949 - regression_loss: 1497.5090 - treatment_acc: 0.4394 - val_loss: 896.8734 - val_tarreg_loss: 892.1360 - val_regression_loss: 444.9877 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 186/100000\n","1/1 [==============================] - 0s 141ms/step - loss: 2997.6365 - tarreg_loss: 2992.8994 - regression_loss: 1496.2108 - treatment_acc: 0.4389 - val_loss: 896.7327 - val_tarreg_loss: 891.9952 - val_regression_loss: 444.9183 - val_treatment_acc: 0.4593 - lr: 1.0000e-07\n","Epoch 187/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2995.0493 - tarreg_loss: 2990.3123 - regression_loss: 1494.9167 - treatment_acc: 0.4389 - val_loss: 896.5939 - val_tarreg_loss: 891.8564 - val_regression_loss: 444.8499 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 188/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2992.4700 - tarreg_loss: 2987.7329 - regression_loss: 1493.6265 - treatment_acc: 0.4394 - val_loss: 896.4570 - val_tarreg_loss: 891.7195 - val_regression_loss: 444.7824 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 189/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2989.8987 - tarreg_loss: 2985.1614 - regression_loss: 1492.3401 - treatment_acc: 0.4389 - val_loss: 896.3219 - val_tarreg_loss: 891.5845 - val_regression_loss: 444.7159 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 190/100000\n","1/1 [==============================] - 0s 130ms/step - loss: 2987.3347 - tarreg_loss: 2982.5974 - regression_loss: 1491.0576 - treatment_acc: 0.4389 - val_loss: 896.1888 - val_tarreg_loss: 891.4514 - val_regression_loss: 444.6503 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 191/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 2984.7781 - tarreg_loss: 2980.0408 - regression_loss: 1489.7788 - treatment_acc: 0.4384 - val_loss: 896.0576 - val_tarreg_loss: 891.3201 - val_regression_loss: 444.5856 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 192/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 2982.2288 - tarreg_loss: 2977.4915 - regression_loss: 1488.5035 - treatment_acc: 0.4389 - val_loss: 895.9281 - val_tarreg_loss: 891.1907 - val_regression_loss: 444.5218 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 193/100000\n","1/1 [==============================] - 0s 136ms/step - loss: 2979.6870 - tarreg_loss: 2974.9497 - regression_loss: 1487.2322 - treatment_acc: 0.4394 - val_loss: 895.8005 - val_tarreg_loss: 891.0631 - val_regression_loss: 444.4590 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 194/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 2977.1523 - tarreg_loss: 2972.4150 - regression_loss: 1485.9645 - treatment_acc: 0.4394 - val_loss: 895.6747 - val_tarreg_loss: 890.9373 - val_regression_loss: 444.3970 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 195/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2974.6252 - tarreg_loss: 2969.8877 - regression_loss: 1484.7002 - treatment_acc: 0.4394 - val_loss: 895.5505 - val_tarreg_loss: 890.8130 - val_regression_loss: 444.3359 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 196/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2972.1050 - tarreg_loss: 2967.3674 - regression_loss: 1483.4395 - treatment_acc: 0.4394 - val_loss: 895.4283 - val_tarreg_loss: 890.6909 - val_regression_loss: 444.2757 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 197/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 2969.5918 - tarreg_loss: 2964.8542 - regression_loss: 1482.1824 - treatment_acc: 0.4394 - val_loss: 895.3077 - val_tarreg_loss: 890.5703 - val_regression_loss: 444.2164 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 198/100000\n","1/1 [==============================] - 0s 114ms/step - loss: 2967.0852 - tarreg_loss: 2962.3477 - regression_loss: 1480.9286 - treatment_acc: 0.4394 - val_loss: 895.1889 - val_tarreg_loss: 890.4515 - val_regression_loss: 444.1578 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 199/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2964.5852 - tarreg_loss: 2959.8477 - regression_loss: 1479.6782 - treatment_acc: 0.4389 - val_loss: 895.0718 - val_tarreg_loss: 890.3344 - val_regression_loss: 444.1002 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 200/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2962.0925 - tarreg_loss: 2957.3550 - regression_loss: 1478.4313 - treatment_acc: 0.4389 - val_loss: 894.9564 - val_tarreg_loss: 890.2189 - val_regression_loss: 444.0435 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 201/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2959.6062 - tarreg_loss: 2954.8684 - regression_loss: 1477.1875 - treatment_acc: 0.4394 - val_loss: 894.8427 - val_tarreg_loss: 890.1052 - val_regression_loss: 443.9875 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 202/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2957.1267 - tarreg_loss: 2952.3889 - regression_loss: 1475.9470 - treatment_acc: 0.4380 - val_loss: 894.7307 - val_tarreg_loss: 889.9932 - val_regression_loss: 443.9324 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 203/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2954.6531 - tarreg_loss: 2949.9153 - regression_loss: 1474.7098 - treatment_acc: 0.4375 - val_loss: 894.6202 - val_tarreg_loss: 889.8827 - val_regression_loss: 443.8781 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 204/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2952.1863 - tarreg_loss: 2947.4485 - regression_loss: 1473.4758 - treatment_acc: 0.4384 - val_loss: 894.5116 - val_tarreg_loss: 889.7740 - val_regression_loss: 443.8246 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 205/100000\n","1/1 [==============================] - 0s 112ms/step - loss: 2949.7251 - tarreg_loss: 2944.9873 - regression_loss: 1472.2449 - treatment_acc: 0.4384 - val_loss: 894.4044 - val_tarreg_loss: 889.6668 - val_regression_loss: 443.7719 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 206/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2947.2710 - tarreg_loss: 2942.5332 - regression_loss: 1471.0172 - treatment_acc: 0.4389 - val_loss: 894.2989 - val_tarreg_loss: 889.5613 - val_regression_loss: 443.7201 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 207/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 2944.8225 - tarreg_loss: 2940.0847 - regression_loss: 1469.7925 - treatment_acc: 0.4389 - val_loss: 894.1949 - val_tarreg_loss: 889.4573 - val_regression_loss: 443.6690 - val_treatment_acc: 0.4611 - lr: 1.0000e-07\n","Epoch 208/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2942.3801 - tarreg_loss: 2937.6423 - regression_loss: 1468.5708 - treatment_acc: 0.4389 - val_loss: 894.0925 - val_tarreg_loss: 889.3549 - val_regression_loss: 443.6187 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 209/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 2939.9441 - tarreg_loss: 2935.2063 - regression_loss: 1467.3522 - treatment_acc: 0.4389 - val_loss: 893.9916 - val_tarreg_loss: 889.2540 - val_regression_loss: 443.5692 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 210/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2937.5137 - tarreg_loss: 2932.7759 - regression_loss: 1466.1367 - treatment_acc: 0.4389 - val_loss: 893.8923 - val_tarreg_loss: 889.1547 - val_regression_loss: 443.5204 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 211/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2935.0891 - tarreg_loss: 2930.3513 - regression_loss: 1464.9238 - treatment_acc: 0.4384 - val_loss: 893.7945 - val_tarreg_loss: 889.0569 - val_regression_loss: 443.4724 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 212/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 2932.6707 - tarreg_loss: 2927.9329 - regression_loss: 1463.7141 - treatment_acc: 0.4384 - val_loss: 893.6982 - val_tarreg_loss: 888.9606 - val_regression_loss: 443.4251 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 213/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 2930.2576 - tarreg_loss: 2925.5198 - regression_loss: 1462.5071 - treatment_acc: 0.4384 - val_loss: 893.6035 - val_tarreg_loss: 888.8659 - val_regression_loss: 443.3787 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 214/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2927.8503 - tarreg_loss: 2923.1125 - regression_loss: 1461.3029 - treatment_acc: 0.4380 - val_loss: 893.5103 - val_tarreg_loss: 888.7726 - val_regression_loss: 443.3329 - val_treatment_acc: 0.4630 - lr: 1.0000e-07\n","Epoch 215/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2925.4485 - tarreg_loss: 2920.7107 - regression_loss: 1460.1016 - treatment_acc: 0.4380 - val_loss: 893.4185 - val_tarreg_loss: 888.6809 - val_regression_loss: 443.2880 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 216/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2923.0522 - tarreg_loss: 2918.3145 - regression_loss: 1458.9030 - treatment_acc: 0.4375 - val_loss: 893.3283 - val_tarreg_loss: 888.5907 - val_regression_loss: 443.2437 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 217/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2920.6611 - tarreg_loss: 2915.9233 - regression_loss: 1457.7070 - treatment_acc: 0.4375 - val_loss: 893.2396 - val_tarreg_loss: 888.5020 - val_regression_loss: 443.2003 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 218/100000\n","1/1 [==============================] - 0s 117ms/step - loss: 2918.2759 - tarreg_loss: 2913.5381 - regression_loss: 1456.5138 - treatment_acc: 0.4380 - val_loss: 893.1523 - val_tarreg_loss: 888.4147 - val_regression_loss: 443.1575 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 219/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 2915.8960 - tarreg_loss: 2911.1582 - regression_loss: 1455.3234 - treatment_acc: 0.4384 - val_loss: 893.0666 - val_tarreg_loss: 888.3288 - val_regression_loss: 443.1154 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 220/100000\n","1/1 [==============================] - 0s 193ms/step - loss: 2913.5212 - tarreg_loss: 2908.7834 - regression_loss: 1454.1355 - treatment_acc: 0.4389 - val_loss: 892.9821 - val_tarreg_loss: 888.2443 - val_regression_loss: 443.0740 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 221/100000\n","1/1 [==============================] - 0s 178ms/step - loss: 2911.1511 - tarreg_loss: 2906.4133 - regression_loss: 1452.9501 - treatment_acc: 0.4394 - val_loss: 892.8990 - val_tarreg_loss: 888.1611 - val_regression_loss: 443.0334 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 222/100000\n","1/1 [==============================] - 0s 204ms/step - loss: 2908.7869 - tarreg_loss: 2904.0491 - regression_loss: 1451.7675 - treatment_acc: 0.4389 - val_loss: 892.8174 - val_tarreg_loss: 888.0795 - val_regression_loss: 442.9934 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 223/100000\n","1/1 [==============================] - 0s 155ms/step - loss: 2906.4277 - tarreg_loss: 2901.6899 - regression_loss: 1450.5874 - treatment_acc: 0.4384 - val_loss: 892.7371 - val_tarreg_loss: 887.9992 - val_regression_loss: 442.9542 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 224/100000\n","1/1 [==============================] - 0s 166ms/step - loss: 2904.0735 - tarreg_loss: 2899.3357 - regression_loss: 1449.4099 - treatment_acc: 0.4389 - val_loss: 892.6582 - val_tarreg_loss: 887.9203 - val_regression_loss: 442.9156 - val_treatment_acc: 0.4648 - lr: 1.0000e-07\n","Epoch 225/100000\n","1/1 [==============================] - 0s 154ms/step - loss: 2901.7241 - tarreg_loss: 2896.9863 - regression_loss: 1448.2346 - treatment_acc: 0.4389 - val_loss: 892.5805 - val_tarreg_loss: 887.8426 - val_regression_loss: 442.8776 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 226/100000\n","1/1 [==============================] - 0s 170ms/step - loss: 2899.3799 - tarreg_loss: 2894.6418 - regression_loss: 1447.0620 - treatment_acc: 0.4389 - val_loss: 892.5043 - val_tarreg_loss: 887.7664 - val_regression_loss: 442.8403 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 227/100000\n","1/1 [==============================] - 0s 133ms/step - loss: 2897.0405 - tarreg_loss: 2892.3025 - regression_loss: 1445.8918 - treatment_acc: 0.4389 - val_loss: 892.4294 - val_tarreg_loss: 887.6915 - val_regression_loss: 442.8038 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 228/100000\n","1/1 [==============================] - 0s 138ms/step - loss: 2894.7061 - tarreg_loss: 2889.9680 - regression_loss: 1444.7241 - treatment_acc: 0.4389 - val_loss: 892.3558 - val_tarreg_loss: 887.6179 - val_regression_loss: 442.7679 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 229/100000\n","1/1 [==============================] - 0s 158ms/step - loss: 2892.3762 - tarreg_loss: 2887.6382 - regression_loss: 1443.5587 - treatment_acc: 0.4394 - val_loss: 892.2838 - val_tarreg_loss: 887.5458 - val_regression_loss: 442.7327 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 230/100000\n","1/1 [==============================] - 0s 164ms/step - loss: 2890.0513 - tarreg_loss: 2885.3132 - regression_loss: 1442.3958 - treatment_acc: 0.4398 - val_loss: 892.2128 - val_tarreg_loss: 887.4748 - val_regression_loss: 442.6981 - val_treatment_acc: 0.4667 - lr: 1.0000e-07\n","Epoch 231/100000\n","1/1 [==============================] - 0s 170ms/step - loss: 2887.7307 - tarreg_loss: 2882.9927 - regression_loss: 1441.2351 - treatment_acc: 0.4398 - val_loss: 892.1431 - val_tarreg_loss: 887.4052 - val_regression_loss: 442.6641 - val_treatment_acc: 0.4685 - lr: 1.0000e-07\n","Epoch 232/100000\n","1/1 [==============================] - 0s 140ms/step - loss: 2885.4155 - tarreg_loss: 2880.6775 - regression_loss: 1440.0769 - treatment_acc: 0.4398 - val_loss: 892.0748 - val_tarreg_loss: 887.3369 - val_regression_loss: 442.6308 - val_treatment_acc: 0.4685 - lr: 1.0000e-07\n","Epoch 233/100000\n","1/1 [==============================] - 0s 139ms/step - loss: 2883.1045 - tarreg_loss: 2878.3665 - regression_loss: 1438.9210 - treatment_acc: 0.4398 - val_loss: 892.0078 - val_tarreg_loss: 887.2698 - val_regression_loss: 442.5981 - val_treatment_acc: 0.4704 - lr: 1.0000e-07\n","Epoch 234/100000\n","1/1 [==============================] - 0s 179ms/step - loss: 2880.7981 - tarreg_loss: 2876.0601 - regression_loss: 1437.7673 - treatment_acc: 0.4403 - val_loss: 891.9421 - val_tarreg_loss: 887.2040 - val_regression_loss: 442.5661 - val_treatment_acc: 0.4704 - lr: 1.0000e-07\n","Epoch 235/100000\n","1/1 [==============================] - 0s 173ms/step - loss: 2878.4961 - tarreg_loss: 2873.7581 - regression_loss: 1436.6158 - treatment_acc: 0.4403 - val_loss: 891.8775 - val_tarreg_loss: 887.1394 - val_regression_loss: 442.5347 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 236/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2876.1987 - tarreg_loss: 2871.4607 - regression_loss: 1435.4668 - treatment_acc: 0.4407 - val_loss: 891.8142 - val_tarreg_loss: 887.0761 - val_regression_loss: 442.5039 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 237/100000\n","1/1 [==============================] - 0s 139ms/step - loss: 2873.9060 - tarreg_loss: 2869.1680 - regression_loss: 1434.3199 - treatment_acc: 0.4403 - val_loss: 891.7520 - val_tarreg_loss: 887.0139 - val_regression_loss: 442.4736 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 238/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 2871.6179 - tarreg_loss: 2866.8799 - regression_loss: 1433.1753 - treatment_acc: 0.4398 - val_loss: 891.6911 - val_tarreg_loss: 886.9529 - val_regression_loss: 442.4440 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 239/100000\n","1/1 [==============================] - 0s 111ms/step - loss: 2869.3337 - tarreg_loss: 2864.5957 - regression_loss: 1432.0327 - treatment_acc: 0.4398 - val_loss: 891.6313 - val_tarreg_loss: 886.8931 - val_regression_loss: 442.4150 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 240/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2867.0544 - tarreg_loss: 2862.3164 - regression_loss: 1430.8926 - treatment_acc: 0.4394 - val_loss: 891.5726 - val_tarreg_loss: 886.8345 - val_regression_loss: 442.3865 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 241/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 2864.7788 - tarreg_loss: 2860.0408 - regression_loss: 1429.7544 - treatment_acc: 0.4394 - val_loss: 891.5153 - val_tarreg_loss: 886.7771 - val_regression_loss: 442.3587 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 242/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2862.5081 - tarreg_loss: 2857.7698 - regression_loss: 1428.6184 - treatment_acc: 0.4394 - val_loss: 891.4589 - val_tarreg_loss: 886.7208 - val_regression_loss: 442.3314 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 243/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2860.2415 - tarreg_loss: 2855.5029 - regression_loss: 1427.4846 - treatment_acc: 0.4394 - val_loss: 891.4038 - val_tarreg_loss: 886.6656 - val_regression_loss: 442.3047 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 244/100000\n","1/1 [==============================] - 0s 108ms/step - loss: 2857.9785 - tarreg_loss: 2853.2400 - regression_loss: 1426.3525 - treatment_acc: 0.4403 - val_loss: 891.3499 - val_tarreg_loss: 886.6117 - val_regression_loss: 442.2786 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 245/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2855.7202 - tarreg_loss: 2850.9817 - regression_loss: 1425.2230 - treatment_acc: 0.4417 - val_loss: 891.2969 - val_tarreg_loss: 886.5588 - val_regression_loss: 442.2530 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 246/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2853.4656 - tarreg_loss: 2848.7271 - regression_loss: 1424.0952 - treatment_acc: 0.4417 - val_loss: 891.2452 - val_tarreg_loss: 886.5070 - val_regression_loss: 442.2280 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 247/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2851.2151 - tarreg_loss: 2846.4766 - regression_loss: 1422.9696 - treatment_acc: 0.4417 - val_loss: 891.1946 - val_tarreg_loss: 886.4564 - val_regression_loss: 442.2036 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 248/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 2848.9690 - tarreg_loss: 2844.2305 - regression_loss: 1421.8459 - treatment_acc: 0.4417 - val_loss: 891.1451 - val_tarreg_loss: 886.4069 - val_regression_loss: 442.1796 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 249/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2846.7268 - tarreg_loss: 2841.9883 - regression_loss: 1420.7244 - treatment_acc: 0.4417 - val_loss: 891.0967 - val_tarreg_loss: 886.3584 - val_regression_loss: 442.1563 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 250/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2844.4880 - tarreg_loss: 2839.7495 - regression_loss: 1419.6046 - treatment_acc: 0.4412 - val_loss: 891.0494 - val_tarreg_loss: 886.3111 - val_regression_loss: 442.1335 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 251/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2842.2532 - tarreg_loss: 2837.5146 - regression_loss: 1418.4868 - treatment_acc: 0.4417 - val_loss: 891.0031 - val_tarreg_loss: 886.2648 - val_regression_loss: 442.1112 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 252/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2840.0227 - tarreg_loss: 2835.2842 - regression_loss: 1417.3711 - treatment_acc: 0.4417 - val_loss: 890.9578 - val_tarreg_loss: 886.2195 - val_regression_loss: 442.0894 - val_treatment_acc: 0.4722 - lr: 1.0000e-07\n","Epoch 253/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 2837.7959 - tarreg_loss: 2833.0574 - regression_loss: 1416.2572 - treatment_acc: 0.4417 - val_loss: 890.9138 - val_tarreg_loss: 886.1754 - val_regression_loss: 442.0683 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 254/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2835.5730 - tarreg_loss: 2830.8345 - regression_loss: 1415.1453 - treatment_acc: 0.4412 - val_loss: 890.8706 - val_tarreg_loss: 886.1323 - val_regression_loss: 442.0475 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 255/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2833.3535 - tarreg_loss: 2828.6150 - regression_loss: 1414.0350 - treatment_acc: 0.4421 - val_loss: 890.8284 - val_tarreg_loss: 886.0901 - val_regression_loss: 442.0273 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 256/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2831.1375 - tarreg_loss: 2826.3989 - regression_loss: 1412.9268 - treatment_acc: 0.4426 - val_loss: 890.7873 - val_tarreg_loss: 886.0490 - val_regression_loss: 442.0077 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 257/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2828.9260 - tarreg_loss: 2824.1875 - regression_loss: 1411.8204 - treatment_acc: 0.4431 - val_loss: 890.7473 - val_tarreg_loss: 886.0089 - val_regression_loss: 441.9885 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 258/100000\n","1/1 [==============================] - 0s 169ms/step - loss: 2826.7175 - tarreg_loss: 2821.9790 - regression_loss: 1410.7158 - treatment_acc: 0.4431 - val_loss: 890.7083 - val_tarreg_loss: 885.9699 - val_regression_loss: 441.9699 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 259/100000\n","1/1 [==============================] - 0s 145ms/step - loss: 2824.5129 - tarreg_loss: 2819.7744 - regression_loss: 1409.6130 - treatment_acc: 0.4426 - val_loss: 890.6703 - val_tarreg_loss: 885.9319 - val_regression_loss: 441.9518 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 260/100000\n","1/1 [==============================] - 0s 106ms/step - loss: 2822.3120 - tarreg_loss: 2817.5735 - regression_loss: 1408.5122 - treatment_acc: 0.4426 - val_loss: 890.6333 - val_tarreg_loss: 885.8948 - val_regression_loss: 441.9341 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 261/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 2820.1145 - tarreg_loss: 2815.3757 - regression_loss: 1407.4130 - treatment_acc: 0.4426 - val_loss: 890.5974 - val_tarreg_loss: 885.8588 - val_regression_loss: 441.9169 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 262/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2817.9204 - tarreg_loss: 2813.1816 - regression_loss: 1406.3154 - treatment_acc: 0.4421 - val_loss: 890.5624 - val_tarreg_loss: 885.8238 - val_regression_loss: 441.9003 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 263/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 2815.7302 - tarreg_loss: 2810.9915 - regression_loss: 1405.2198 - treatment_acc: 0.4421 - val_loss: 890.5281 - val_tarreg_loss: 885.7896 - val_regression_loss: 441.8840 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 264/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2813.5432 - tarreg_loss: 2808.8044 - regression_loss: 1404.1260 - treatment_acc: 0.4421 - val_loss: 890.4951 - val_tarreg_loss: 885.7565 - val_regression_loss: 441.8684 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 265/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 2811.3596 - tarreg_loss: 2806.6208 - regression_loss: 1403.0337 - treatment_acc: 0.4417 - val_loss: 890.4630 - val_tarreg_loss: 885.7244 - val_regression_loss: 441.8532 - val_treatment_acc: 0.4741 - lr: 1.0000e-07\n","Epoch 266/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2809.1792 - tarreg_loss: 2804.4404 - regression_loss: 1401.9431 - treatment_acc: 0.4417 - val_loss: 890.4318 - val_tarreg_loss: 885.6932 - val_regression_loss: 441.8385 - val_treatment_acc: 0.4759 - lr: 1.0000e-07\n","Epoch 267/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2807.0024 - tarreg_loss: 2802.2637 - regression_loss: 1400.8544 - treatment_acc: 0.4417 - val_loss: 890.4017 - val_tarreg_loss: 885.6631 - val_regression_loss: 441.8243 - val_treatment_acc: 0.4759 - lr: 1.0000e-07\n","Epoch 268/100000\n","1/1 [==============================] - 0s 116ms/step - loss: 2804.8289 - tarreg_loss: 2800.0901 - regression_loss: 1399.7671 - treatment_acc: 0.4421 - val_loss: 890.3724 - val_tarreg_loss: 885.6337 - val_regression_loss: 441.8105 - val_treatment_acc: 0.4759 - lr: 1.0000e-07\n","Epoch 269/100000\n","1/1 [==============================] - 0s 144ms/step - loss: 2802.6587 - tarreg_loss: 2797.9199 - regression_loss: 1398.6815 - treatment_acc: 0.4431 - val_loss: 890.3442 - val_tarreg_loss: 885.6055 - val_regression_loss: 441.7973 - val_treatment_acc: 0.4759 - lr: 1.0000e-07\n","Epoch 270/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 2800.4917 - tarreg_loss: 2795.7529 - regression_loss: 1397.5977 - treatment_acc: 0.4426 - val_loss: 890.3168 - val_tarreg_loss: 885.5782 - val_regression_loss: 441.7845 - val_treatment_acc: 0.4759 - lr: 1.0000e-07\n","Epoch 271/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2798.3279 - tarreg_loss: 2793.5891 - regression_loss: 1396.5154 - treatment_acc: 0.4426 - val_loss: 890.2906 - val_tarreg_loss: 885.5518 - val_regression_loss: 441.7722 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 272/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2796.1672 - tarreg_loss: 2791.4285 - regression_loss: 1395.4346 - treatment_acc: 0.4426 - val_loss: 890.2651 - val_tarreg_loss: 885.5263 - val_regression_loss: 441.7603 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 273/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2794.0100 - tarreg_loss: 2789.2712 - regression_loss: 1394.3557 - treatment_acc: 0.4426 - val_loss: 890.2405 - val_tarreg_loss: 885.5018 - val_regression_loss: 441.7489 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 274/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2791.8557 - tarreg_loss: 2787.1169 - regression_loss: 1393.2781 - treatment_acc: 0.4426 - val_loss: 890.2169 - val_tarreg_loss: 885.4781 - val_regression_loss: 441.7379 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 275/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2789.7048 - tarreg_loss: 2784.9661 - regression_loss: 1392.2021 - treatment_acc: 0.4421 - val_loss: 890.1940 - val_tarreg_loss: 885.4552 - val_regression_loss: 441.7274 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 276/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 2787.5566 - tarreg_loss: 2782.8179 - regression_loss: 1391.1277 - treatment_acc: 0.4426 - val_loss: 890.1722 - val_tarreg_loss: 885.4334 - val_regression_loss: 441.7173 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 277/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2785.4116 - tarreg_loss: 2780.6729 - regression_loss: 1390.0549 - treatment_acc: 0.4421 - val_loss: 890.1513 - val_tarreg_loss: 885.4125 - val_regression_loss: 441.7077 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 278/100000\n","1/1 [==============================] - 0s 138ms/step - loss: 2783.2700 - tarreg_loss: 2778.5310 - regression_loss: 1388.9834 - treatment_acc: 0.4417 - val_loss: 890.1311 - val_tarreg_loss: 885.3923 - val_regression_loss: 441.6985 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 279/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2781.1311 - tarreg_loss: 2776.3921 - regression_loss: 1387.9136 - treatment_acc: 0.4417 - val_loss: 890.1121 - val_tarreg_loss: 885.3732 - val_regression_loss: 441.6898 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 280/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2778.9951 - tarreg_loss: 2774.2561 - regression_loss: 1386.8452 - treatment_acc: 0.4421 - val_loss: 890.0938 - val_tarreg_loss: 885.3549 - val_regression_loss: 441.6815 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 281/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2776.8618 - tarreg_loss: 2772.1228 - regression_loss: 1385.7782 - treatment_acc: 0.4421 - val_loss: 890.0764 - val_tarreg_loss: 885.3375 - val_regression_loss: 441.6737 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 282/100000\n","1/1 [==============================] - 0s 142ms/step - loss: 2774.7314 - tarreg_loss: 2769.9924 - regression_loss: 1384.7126 - treatment_acc: 0.4421 - val_loss: 890.0599 - val_tarreg_loss: 885.3210 - val_regression_loss: 441.6663 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 283/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2772.6045 - tarreg_loss: 2767.8655 - regression_loss: 1383.6487 - treatment_acc: 0.4426 - val_loss: 890.0443 - val_tarreg_loss: 885.3054 - val_regression_loss: 441.6594 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 284/100000\n","1/1 [==============================] - 0s 122ms/step - loss: 2770.4797 - tarreg_loss: 2765.7407 - regression_loss: 1382.5859 - treatment_acc: 0.4421 - val_loss: 890.0295 - val_tarreg_loss: 885.2906 - val_regression_loss: 441.6528 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 285/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 2768.3584 - tarreg_loss: 2763.6194 - regression_loss: 1381.5249 - treatment_acc: 0.4421 - val_loss: 890.0156 - val_tarreg_loss: 885.2766 - val_regression_loss: 441.6468 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 286/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2766.2393 - tarreg_loss: 2761.5002 - regression_loss: 1380.4650 - treatment_acc: 0.4412 - val_loss: 890.0026 - val_tarreg_loss: 885.2635 - val_regression_loss: 441.6411 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 287/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2764.1233 - tarreg_loss: 2759.3843 - regression_loss: 1379.4065 - treatment_acc: 0.4412 - val_loss: 889.9904 - val_tarreg_loss: 885.2513 - val_regression_loss: 441.6358 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 288/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2762.0100 - tarreg_loss: 2757.2710 - regression_loss: 1378.3495 - treatment_acc: 0.4407 - val_loss: 889.9790 - val_tarreg_loss: 885.2400 - val_regression_loss: 441.6310 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 289/100000\n","1/1 [==============================] - 0s 118ms/step - loss: 2759.8992 - tarreg_loss: 2755.1602 - regression_loss: 1377.2937 - treatment_acc: 0.4403 - val_loss: 889.9683 - val_tarreg_loss: 885.2293 - val_regression_loss: 441.6265 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 290/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 2757.7913 - tarreg_loss: 2753.0522 - regression_loss: 1376.2393 - treatment_acc: 0.4403 - val_loss: 889.9587 - val_tarreg_loss: 885.2197 - val_regression_loss: 441.6226 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 291/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2755.6860 - tarreg_loss: 2750.9470 - regression_loss: 1375.1863 - treatment_acc: 0.4403 - val_loss: 889.9498 - val_tarreg_loss: 885.2108 - val_regression_loss: 441.6190 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 292/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 2753.5830 - tarreg_loss: 2748.8440 - regression_loss: 1374.1346 - treatment_acc: 0.4403 - val_loss: 889.9417 - val_tarreg_loss: 885.2026 - val_regression_loss: 441.6158 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 293/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2751.4834 - tarreg_loss: 2746.7441 - regression_loss: 1373.0842 - treatment_acc: 0.4403 - val_loss: 889.9343 - val_tarreg_loss: 885.1951 - val_regression_loss: 441.6129 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 294/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 2749.3860 - tarreg_loss: 2744.6467 - regression_loss: 1372.0352 - treatment_acc: 0.4407 - val_loss: 889.9280 - val_tarreg_loss: 885.1887 - val_regression_loss: 441.6105 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 295/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 2747.2913 - tarreg_loss: 2742.5520 - regression_loss: 1370.9873 - treatment_acc: 0.4407 - val_loss: 889.9221 - val_tarreg_loss: 885.1828 - val_regression_loss: 441.6084 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 296/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2745.1987 - tarreg_loss: 2740.4595 - regression_loss: 1369.9407 - treatment_acc: 0.4407 - val_loss: 889.9172 - val_tarreg_loss: 885.1780 - val_regression_loss: 441.6068 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 297/100000\n","1/1 [==============================] - 0s 129ms/step - loss: 2743.1089 - tarreg_loss: 2738.3696 - regression_loss: 1368.8954 - treatment_acc: 0.4407 - val_loss: 889.9129 - val_tarreg_loss: 885.1736 - val_regression_loss: 441.6055 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 298/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2741.0215 - tarreg_loss: 2736.2822 - regression_loss: 1367.8513 - treatment_acc: 0.4417 - val_loss: 889.9095 - val_tarreg_loss: 885.1702 - val_regression_loss: 441.6047 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 299/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2738.9368 - tarreg_loss: 2734.1975 - regression_loss: 1366.8086 - treatment_acc: 0.4426 - val_loss: 889.9068 - val_tarreg_loss: 885.1675 - val_regression_loss: 441.6042 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 300/100000\n","1/1 [==============================] - 0s 136ms/step - loss: 2736.8545 - tarreg_loss: 2732.1152 - regression_loss: 1365.7671 - treatment_acc: 0.4431 - val_loss: 889.9049 - val_tarreg_loss: 885.1656 - val_regression_loss: 441.6041 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 301/100000\n","1/1 [==============================] - 0s 115ms/step - loss: 2734.7747 - tarreg_loss: 2730.0354 - regression_loss: 1364.7268 - treatment_acc: 0.4431 - val_loss: 889.9038 - val_tarreg_loss: 885.1645 - val_regression_loss: 441.6044 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 302/100000\n","1/1 [==============================] - 0s 124ms/step - loss: 2732.6965 - tarreg_loss: 2727.9573 - regression_loss: 1363.6876 - treatment_acc: 0.4431 - val_loss: 889.9034 - val_tarreg_loss: 885.1641 - val_regression_loss: 441.6051 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 303/100000\n","1/1 [==============================] - 0s 108ms/step - loss: 2730.6216 - tarreg_loss: 2725.8823 - regression_loss: 1362.6497 - treatment_acc: 0.4431 - val_loss: 889.9037 - val_tarreg_loss: 885.1644 - val_regression_loss: 441.6061 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 304/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2728.5491 - tarreg_loss: 2723.8098 - regression_loss: 1361.6130 - treatment_acc: 0.4435 - val_loss: 889.9050 - val_tarreg_loss: 885.1656 - val_regression_loss: 441.6075 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 305/100000\n","1/1 [==============================] - 0s 125ms/step - loss: 2726.4790 - tarreg_loss: 2721.7397 - regression_loss: 1360.5776 - treatment_acc: 0.4435 - val_loss: 889.9069 - val_tarreg_loss: 885.1674 - val_regression_loss: 441.6093 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 306/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2724.4109 - tarreg_loss: 2719.6716 - regression_loss: 1359.5431 - treatment_acc: 0.4440 - val_loss: 889.9096 - val_tarreg_loss: 885.1701 - val_regression_loss: 441.6115 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 307/100000\n","1/1 [==============================] - 0s 120ms/step - loss: 2722.3450 - tarreg_loss: 2717.6057 - regression_loss: 1358.5098 - treatment_acc: 0.4440 - val_loss: 889.9129 - val_tarreg_loss: 885.1734 - val_regression_loss: 441.6140 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 308/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 2720.2815 - tarreg_loss: 2715.5422 - regression_loss: 1357.4778 - treatment_acc: 0.4440 - val_loss: 889.9171 - val_tarreg_loss: 885.1776 - val_regression_loss: 441.6169 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 309/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2718.2202 - tarreg_loss: 2713.4810 - regression_loss: 1356.4468 - treatment_acc: 0.4431 - val_loss: 889.9219 - val_tarreg_loss: 885.1823 - val_regression_loss: 441.6201 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 310/100000\n","1/1 [==============================] - 0s 139ms/step - loss: 2716.1614 - tarreg_loss: 2711.4221 - regression_loss: 1355.4170 - treatment_acc: 0.4426 - val_loss: 889.9274 - val_tarreg_loss: 885.1878 - val_regression_loss: 441.6237 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 311/100000\n","1/1 [==============================] - 0s 128ms/step - loss: 2714.1045 - tarreg_loss: 2709.3652 - regression_loss: 1354.3882 - treatment_acc: 0.4431 - val_loss: 889.9335 - val_tarreg_loss: 885.1940 - val_regression_loss: 441.6276 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 312/100000\n","1/1 [==============================] - 0s 127ms/step - loss: 2712.0503 - tarreg_loss: 2707.3110 - regression_loss: 1353.3606 - treatment_acc: 0.4426 - val_loss: 889.9404 - val_tarreg_loss: 885.2009 - val_regression_loss: 441.6319 - val_treatment_acc: 0.4815 - lr: 1.0000e-07\n","Epoch 313/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2709.9980 - tarreg_loss: 2705.2585 - regression_loss: 1352.3340 - treatment_acc: 0.4426 - val_loss: 889.9481 - val_tarreg_loss: 885.2085 - val_regression_loss: 441.6366 - val_treatment_acc: 0.4815 - lr: 1.0000e-07\n","Epoch 314/100000\n","1/1 [==============================] - 0s 140ms/step - loss: 2707.9480 - tarreg_loss: 2703.2085 - regression_loss: 1351.3087 - treatment_acc: 0.4426 - val_loss: 889.9563 - val_tarreg_loss: 885.2167 - val_regression_loss: 441.6415 - val_treatment_acc: 0.4815 - lr: 1.0000e-07\n","Epoch 315/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 2705.8999 - tarreg_loss: 2701.1604 - regression_loss: 1350.2843 - treatment_acc: 0.4426 - val_loss: 889.9653 - val_tarreg_loss: 885.2257 - val_regression_loss: 441.6469 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 316/100000\n","1/1 [==============================] - 0s 139ms/step - loss: 2703.8540 - tarreg_loss: 2699.1145 - regression_loss: 1349.2610 - treatment_acc: 0.4435 - val_loss: 889.9749 - val_tarreg_loss: 885.2353 - val_regression_loss: 441.6525 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 317/100000\n","1/1 [==============================] - 0s 166ms/step - loss: 2701.8103 - tarreg_loss: 2697.0708 - regression_loss: 1348.2388 - treatment_acc: 0.4435 - val_loss: 889.9852 - val_tarreg_loss: 885.2456 - val_regression_loss: 441.6585 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 318/100000\n","1/1 [==============================] - 0s 142ms/step - loss: 2699.7686 - tarreg_loss: 2695.0291 - regression_loss: 1347.2175 - treatment_acc: 0.4435 - val_loss: 889.9963 - val_tarreg_loss: 885.2566 - val_regression_loss: 441.6648 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 319/100000\n","1/1 [==============================] - 0s 219ms/step - loss: 2697.7288 - tarreg_loss: 2692.9893 - regression_loss: 1346.1974 - treatment_acc: 0.4431 - val_loss: 890.0080 - val_tarreg_loss: 885.2682 - val_regression_loss: 441.6715 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 320/100000\n","1/1 [==============================] - 0s 210ms/step - loss: 2695.6912 - tarreg_loss: 2690.9517 - regression_loss: 1345.1782 - treatment_acc: 0.4426 - val_loss: 890.0204 - val_tarreg_loss: 885.2806 - val_regression_loss: 441.6785 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 321/100000\n","1/1 [==============================] - 0s 152ms/step - loss: 2693.6560 - tarreg_loss: 2688.9163 - regression_loss: 1344.1602 - treatment_acc: 0.4426 - val_loss: 890.0335 - val_tarreg_loss: 885.2937 - val_regression_loss: 441.6859 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 322/100000\n","1/1 [==============================] - 0s 173ms/step - loss: 2691.6226 - tarreg_loss: 2686.8828 - regression_loss: 1343.1431 - treatment_acc: 0.4426 - val_loss: 890.0472 - val_tarreg_loss: 885.3074 - val_regression_loss: 441.6935 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 323/100000\n","1/1 [==============================] - 0s 146ms/step - loss: 2689.5908 - tarreg_loss: 2684.8511 - regression_loss: 1342.1270 - treatment_acc: 0.4426 - val_loss: 890.0616 - val_tarreg_loss: 885.3217 - val_regression_loss: 441.7015 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 324/100000\n","1/1 [==============================] - 0s 159ms/step - loss: 2687.5615 - tarreg_loss: 2682.8218 - regression_loss: 1341.1118 - treatment_acc: 0.4426 - val_loss: 890.0765 - val_tarreg_loss: 885.3367 - val_regression_loss: 441.7098 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 325/100000\n","1/1 [==============================] - 0s 150ms/step - loss: 2685.5339 - tarreg_loss: 2680.7942 - regression_loss: 1340.0979 - treatment_acc: 0.4431 - val_loss: 890.0923 - val_tarreg_loss: 885.3524 - val_regression_loss: 441.7186 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 326/100000\n","1/1 [==============================] - 0s 198ms/step - loss: 2683.5083 - tarreg_loss: 2678.7686 - regression_loss: 1339.0847 - treatment_acc: 0.4431 - val_loss: 890.1088 - val_tarreg_loss: 885.3689 - val_regression_loss: 441.7276 - val_treatment_acc: 0.4796 - lr: 1.0000e-07\n","Epoch 327/100000\n","1/1 [==============================] - 0s 181ms/step - loss: 2681.4846 - tarreg_loss: 2676.7449 - regression_loss: 1338.0725 - treatment_acc: 0.4431 - val_loss: 890.1257 - val_tarreg_loss: 885.3858 - val_regression_loss: 441.7369 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 328/100000\n","1/1 [==============================] - 0s 156ms/step - loss: 2679.4631 - tarreg_loss: 2674.7234 - regression_loss: 1337.0613 - treatment_acc: 0.4426 - val_loss: 890.1434 - val_tarreg_loss: 885.4034 - val_regression_loss: 441.7466 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 329/100000\n","1/1 [==============================] - 0s 154ms/step - loss: 2677.4434 - tarreg_loss: 2672.7036 - regression_loss: 1336.0510 - treatment_acc: 0.4431 - val_loss: 890.1618 - val_tarreg_loss: 885.4218 - val_regression_loss: 441.7566 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 330/100000\n","1/1 [==============================] - 0s 159ms/step - loss: 2675.4250 - tarreg_loss: 2670.6853 - regression_loss: 1335.0417 - treatment_acc: 0.4431 - val_loss: 890.1808 - val_tarreg_loss: 885.4408 - val_regression_loss: 441.7669 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 331/100000\n","1/1 [==============================] - 0s 155ms/step - loss: 2673.4092 - tarreg_loss: 2668.6694 - regression_loss: 1334.0333 - treatment_acc: 0.4431 - val_loss: 890.2004 - val_tarreg_loss: 885.4603 - val_regression_loss: 441.7775 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 332/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2671.3950 - tarreg_loss: 2666.6553 - regression_loss: 1333.0260 - treatment_acc: 0.4431 - val_loss: 890.2206 - val_tarreg_loss: 885.4805 - val_regression_loss: 441.7884 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 333/100000\n","1/1 [==============================] - 0s 113ms/step - loss: 2669.3828 - tarreg_loss: 2664.6431 - regression_loss: 1332.0195 - treatment_acc: 0.4431 - val_loss: 890.2414 - val_tarreg_loss: 885.5013 - val_regression_loss: 441.7996 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 334/100000\n","1/1 [==============================] - 0s 123ms/step - loss: 2667.3721 - tarreg_loss: 2662.6323 - regression_loss: 1331.0139 - treatment_acc: 0.4431 - val_loss: 890.2630 - val_tarreg_loss: 885.5229 - val_regression_loss: 441.8112 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 335/100000\n","1/1 [==============================] - 0s 121ms/step - loss: 2665.3633 - tarreg_loss: 2660.6235 - regression_loss: 1330.0092 - treatment_acc: 0.4435 - val_loss: 890.2851 - val_tarreg_loss: 885.5449 - val_regression_loss: 441.8231 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 336/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2663.3564 - tarreg_loss: 2658.6165 - regression_loss: 1329.0054 - treatment_acc: 0.4435 - val_loss: 890.3078 - val_tarreg_loss: 885.5676 - val_regression_loss: 441.8353 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 337/100000\n","1/1 [==============================] - 0s 119ms/step - loss: 2661.3516 - tarreg_loss: 2656.6113 - regression_loss: 1328.0024 - treatment_acc: 0.4435 - val_loss: 890.3311 - val_tarreg_loss: 885.5909 - val_regression_loss: 441.8477 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 338/100000\n","1/1 [==============================] - 0s 126ms/step - loss: 2659.3479 - tarreg_loss: 2654.6077 - regression_loss: 1327.0004 - treatment_acc: 0.4431 - val_loss: 890.3551 - val_tarreg_loss: 885.6149 - val_regression_loss: 441.8605 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 339/100000\n","1/1 [==============================] - 0s 131ms/step - loss: 2657.3464 - tarreg_loss: 2652.6062 - regression_loss: 1325.9993 - treatment_acc: 0.4431 - val_loss: 890.3796 - val_tarreg_loss: 885.6395 - val_regression_loss: 441.8737 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 340/100000\n","1/1 [==============================] - 0s 137ms/step - loss: 2655.3462 - tarreg_loss: 2650.6060 - regression_loss: 1324.9988 - treatment_acc: 0.4431 - val_loss: 890.4049 - val_tarreg_loss: 885.6647 - val_regression_loss: 441.8871 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 341/100000\n","1/1 [==============================] - 0s 136ms/step - loss: 2653.3479 - tarreg_loss: 2648.6077 - regression_loss: 1323.9993 - treatment_acc: 0.4431 - val_loss: 890.4307 - val_tarreg_loss: 885.6904 - val_regression_loss: 441.9008 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","Epoch 342/100000\n","1/1 [==============================] - 0s 135ms/step - loss: 2651.3511 - tarreg_loss: 2646.6108 - regression_loss: 1323.0006 - treatment_acc: 0.4431 - val_loss: 890.4571 - val_tarreg_loss: 885.7168 - val_regression_loss: 441.9148 - val_treatment_acc: 0.4778 - lr: 1.0000e-07\n","85/85 [==============================] - 0s 2ms/step\n","10/10 [==============================] - 0s 2ms/step\n","tf.Tensor([300 300], shape=(2,), dtype=int32)\n"]}],"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n","from tensorflow.keras.optimizers import SGD, Adam\n","import io\n","from IPython.display import display, clear_output\n","#Colab command to allow us to run Colab in TF2\n","%load_ext tensorboard\n","# data=get_news_data(1)\n","val_split=0.20\n","batch_size=4000\n","verbose=1\n","i = 0\n","tf.random.set_seed(i)\n","np.random.seed(i)\n","!rm -rf ./logs_dragonnet/\n","sim_evals = []\n","for j in range(0,50):\n","  clear_output(wait=True)\n","  print(j+1)\n","  data_train, data_test = get_HMINIST_data(j+1)\n","  yt = np.concatenate([data_train['ys'], data_train['t']], 1)\n","\n","  # Clear any logs from previous runs\n","\n","  start_time = time.time()\n","\n","  log_dir = \"logs_dragonnet/fit/\" +str(j)+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n","  file_writer.set_as_default()\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n","\n","\n","  sgd_callbacks = [\n","        TerminateOnNaN(),\n","        EarlyStopping(monitor='val_loss', patience=20, min_delta=0.),\n","        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n","                          min_delta=0., cooldown=0, min_lr=0),\n","        tensorboard_callback]\n","\n","  sgd_lr = 0.0000001\n","  momentum = 0.9\n","\n","  dragonnet_model=make_dragonnet(data_train['x'].shape[1],.01)\n","  tarreg_loss=TarReg_Loss(alpha=1)\n","\n","  dragonnet_model.compile(optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n","                      loss=tarreg_loss,\n","                 metrics=[tarreg_loss,tarreg_loss.regression_loss,tarreg_loss.treatment_acc])\n","\n","  dragonnet_model.fit(x=data_train['x'],y=yt,\n","                 callbacks=sgd_callbacks,\n","                  validation_split=val_split,\n","                  epochs=100000,\n","                  batch_size=batch_size,\n","                  verbose=verbose)\n","  end_time = time.time()\n","\n","  elapsed_time_seconds = end_time - start_time\n","  # Convert elapsed time to minutes\n","  elapsed_time_minutes = elapsed_time_seconds / 60\n","\n","  Evaluation_metrics_insample = Eval_metrics_train(data_train)\n","  concat_pred_insample = dragonnet_model.predict(data_train['x'])\n","  ATE_abs_insample = Evaluation_metrics_insample.ATE_absolute_error(concat_pred_insample)\n","  ATE_TARREG_insample = Evaluation_metrics_insample.TARREG_ATE_absolute_error(concat_pred_insample)\n","  ITE_RMSE_insample = Evaluation_metrics_insample.ITE_RMSE_error(concat_pred_insample)\n","\n","  Evaluation_metrics_outsample = Eval_metrics_test(data_test)\n","  concat_pred_outsample = dragonnet_model.predict(data_test['x'])\n","  ATE_TARREG_outsample = Evaluation_metrics_outsample.TARREG_ATE_absolute_error(concat_pred_outsample)\n","  PEHE_outsample = Evaluation_metrics_outsample.PEHE(concat_pred_outsample)\n","  PEHE_tareg_outsample = Evaluation_metrics_outsample.PEHE_tareg(concat_pred_outsample)\n","\n","  metrics = [ATE_abs_insample.numpy(), ATE_TARREG_insample.numpy(), ITE_RMSE_insample.numpy(), ATE_TARREG_outsample.numpy(), PEHE_outsample.numpy(), PEHE_tareg_outsample.numpy(), elapsed_time_minutes]\n","  sim_evals.append(metrics)"]},{"cell_type":"code","execution_count":null,"id":"01a697a4","metadata":{"id":"01a697a4"},"outputs":[],"source":["sim_evals = np.asarray(sim_evals)\n","# with open('eval_metrics_news_20.npy', 'wb') as f:\n","#     np.save(f,sim_evals)"]},{"cell_type":"code","execution_count":null,"id":"dda9573c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"dda9573c","executionInfo":{"status":"error","timestamp":1695425824599,"user_tz":240,"elapsed":5,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"98d2e9a9-bb64-4c2f-d28e-85167192a82d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-49dd3474aed4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [ATE_abs_insample.numpy(), ATE_TARREG_insample.numpy(), ITE_RMSE_insample.numpy(), ATE_TARREG_outsample.numpy(), PEHE_outsample.numpy(), PEHE_tareg_outsample.numpy(), elapsed_time_minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Insample ATE error for Dragonnet(mean over 50) =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"+-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Insample ATE targ_reg error for Dragonnet(mean over 50) =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"+-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Insample ITE_RMSE error for Dragonnet(mean over 50) =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"+-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_evals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["# [ATE_abs_insample.numpy(), ATE_TARREG_insample.numpy(), ITE_RMSE_insample.numpy(), ATE_TARREG_outsample.numpy(), PEHE_outsample.numpy(), PEHE_tareg_outsample.numpy(), elapsed_time_minutes\n","\n","print(\" Insample ATE error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,0]),2),\"+-\",round((np.std(sim_evals[:,0], ddof=1) / np.sqrt(np.size(sim_evals[:,0]))),2))\n","print(\" Insample ATE targ_reg error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,1]),2),\"+-\",round((np.std(sim_evals[:,1], ddof=1) / np.sqrt(np.size(sim_evals[:,1]))),2))\n","print(\" Insample ITE_RMSE error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,2]),2),\"+-\",round((np.std(sim_evals[:,2], ddof=1) / np.sqrt(np.size(sim_evals[:,2]))),2))\n","print(\" Outsample ATE targ_reg error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,3]),2),\"+-\",round((np.std(sim_evals[:,3], ddof=1) / np.sqrt(np.size(sim_evals[:,3]))),2))\n","print(\" Outsample PEHE error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,4]),2),\"+-\",round((np.std(sim_evals[:,4], ddof=1) / np.sqrt(np.size(sim_evals[:,4]))),2))\n","print(\" Outsample PEHE targ_reg error for Dragonnet(mean over 50) =\", round(np.mean(sim_evals[:,5]),2),\"+-\",round((np.std(sim_evals[:,5], ddof=1) / np.sqrt(np.size(sim_evals[:,5]))),2))\n","print(\" Wall time for Dragonnet(sum over 50) =\", np.sum(sim_evals[:,6]))"]},{"cell_type":"code","execution_count":null,"id":"89e2239e","metadata":{"id":"89e2239e"},"outputs":[],"source":["from tensorflow.python.client import device_lib\n","\n","device_lib.list_local_devices()"]},{"cell_type":"code","execution_count":null,"id":"a901910a","metadata":{"id":"a901910a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}